04/22 04:25:42 PM: Printing arguments : Namespace(checkpoint='best_model_dir_1234/baseline_best.pth', senteval_vocab=True, encoder='baseline', snli=True, senteval=True, seed=1111, num_workers=4, batch_size=64, sent_eval_path='./SentEval/data', tokenize=False)
04/22 04:25:42 PM: Setting seed...
04/22 04:25:42 PM: Vocab already exists. Loading from disk...
04/22 04:25:50 PM: Loading the model checkpoint from best_model_dir_1234/baseline_best.pth
04/22 04:25:50 PM: Loading the model checkpoint trained in SNLI dataset
04/22 04:25:51 PM: Evaluating the model on SNLI dataset
04/22 04:25:51 PM: Batch: 1/154, Loss: 0.7540, Acc: 64.0625
04/22 04:25:51 PM: Batch: 11/154, Loss: 0.7281, Acc: 68.7500
04/22 04:25:51 PM: Batch: 21/154, Loss: 0.8359, Acc: 56.2500
04/22 04:25:51 PM: Batch: 31/154, Loss: 0.6027, Acc: 78.1250
04/22 04:25:51 PM: Batch: 41/154, Loss: 0.7169, Acc: 73.4375
04/22 04:25:51 PM: Batch: 51/154, Loss: 0.8181, Acc: 60.9375
04/22 04:25:51 PM: Batch: 61/154, Loss: 0.8406, Acc: 56.2500
04/22 04:25:51 PM: Batch: 71/154, Loss: 0.8812, Acc: 56.2500
04/22 04:25:51 PM: Batch: 81/154, Loss: 0.8277, Acc: 68.7500
04/22 04:25:51 PM: Batch: 91/154, Loss: 0.7731, Acc: 68.7500
04/22 04:25:51 PM: Batch: 101/154, Loss: 0.7951, Acc: 68.7500
04/22 04:25:51 PM: Batch: 111/154, Loss: 0.6731, Acc: 71.8750
04/22 04:25:51 PM: Batch: 121/154, Loss: 0.7259, Acc: 68.7500
04/22 04:25:51 PM: Batch: 131/154, Loss: 0.9434, Acc: 59.3750
04/22 04:25:51 PM: Batch: 141/154, Loss: 0.7814, Acc: 67.1875
04/22 04:25:51 PM: Batch: 151/154, Loss: 0.7647, Acc: 70.3125
04/22 04:25:52 PM: Batch: 1/154, Loss: 0.7939, Acc: 64.0625
04/22 04:25:52 PM: Batch: 11/154, Loss: 0.7908, Acc: 64.0625
04/22 04:25:52 PM: Batch: 21/154, Loss: 0.7190, Acc: 76.5625
04/22 04:25:52 PM: Batch: 31/154, Loss: 0.7624, Acc: 70.3125
04/22 04:25:52 PM: Batch: 41/154, Loss: 0.7618, Acc: 60.9375
04/22 04:25:52 PM: Batch: 51/154, Loss: 0.8279, Acc: 62.5000
04/22 04:25:52 PM: Batch: 61/154, Loss: 0.7173, Acc: 65.6250
04/22 04:25:52 PM: Batch: 71/154, Loss: 0.7320, Acc: 67.1875
04/22 04:25:52 PM: Batch: 81/154, Loss: 0.8302, Acc: 64.0625
04/22 04:25:52 PM: Batch: 91/154, Loss: 0.7427, Acc: 68.7500
04/22 04:25:52 PM: Batch: 101/154, Loss: 0.8435, Acc: 65.6250
04/22 04:25:52 PM: Batch: 111/154, Loss: 0.8206, Acc: 65.6250
04/22 04:25:52 PM: Batch: 121/154, Loss: 0.7490, Acc: 67.1875
04/22 04:25:52 PM: Batch: 131/154, Loss: 0.8047, Acc: 67.1875
04/22 04:25:52 PM: Batch: 141/154, Loss: 0.8268, Acc: 67.1875
04/22 04:25:52 PM: Batch: 151/154, Loss: 0.8763, Acc: 62.5000
04/22 04:25:52 PM: Validation loss: 0.7840, Validation accuracy: 66.0536
04/22 04:25:52 PM: Test loss: 0.7885, Test accuracy: 66.1950
04/22 04:25:52 PM: Evaluating the model on SentEval tasks
04/22 04:25:52 PM: Building vocabulary for task MR...
04/22 04:25:52 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:25:54 PM: Building unique tokens of vocab based on MR..
04/22 04:25:54 PM: Building w2i and aligned embeddings for task MR
04/22 04:25:55 PM: Generating sentence embeddings
04/22 04:25:55 PM: Generated sentence embeddings
04/22 04:25:55 PM: Training pytorch-MLP-nhid0-adam-bs64 with (inner) 5-fold cross-validation
04/22 04:27:02 PM: Best param found at split 1: l2reg = 1e-05                 with score 77.73
04/22 04:28:19 PM: Best param found at split 2: l2reg = 0.0001                 with score 77.91
04/22 04:29:35 PM: Best param found at split 3: l2reg = 1e-05                 with score 77.47
04/22 04:30:52 PM: Best param found at split 4: l2reg = 0.0001                 with score 77.69
04/22 04:32:05 PM: Best param found at split 5: l2reg = 0.0001                 with score 77.67
04/22 04:32:08 PM: Building vocabulary for task CR...
04/22 04:32:08 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:32:10 PM: Building unique tokens of vocab based on CR..
04/22 04:32:10 PM: Building w2i and aligned embeddings for task CR
04/22 04:32:10 PM: Generating sentence embeddings
04/22 04:32:11 PM: Generated sentence embeddings
04/22 04:32:11 PM: Training pytorch-MLP-nhid0-adam-bs64 with (inner) 5-fold cross-validation
04/22 04:32:39 PM: Best param found at split 1: l2reg = 1e-05                 with score 79.57
04/22 04:33:09 PM: Best param found at split 2: l2reg = 0.0001                 with score 79.6
04/22 04:33:37 PM: Best param found at split 3: l2reg = 1e-05                 with score 79.07
04/22 04:34:08 PM: Best param found at split 4: l2reg = 1e-05                 with score 79.24
04/22 04:34:38 PM: Best param found at split 5: l2reg = 1e-05                 with score 79.93
04/22 04:34:40 PM: Building vocabulary for task MPQA...
04/22 04:34:40 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:34:42 PM: Building unique tokens of vocab based on MPQA..
04/22 04:34:42 PM: Building w2i and aligned embeddings for task MPQA
04/22 04:34:42 PM: Generating sentence embeddings
04/22 04:34:42 PM: Generated sentence embeddings
04/22 04:34:42 PM: Training pytorch-MLP-nhid0-adam-bs64 with (inner) 5-fold cross-validation
04/22 04:35:44 PM: Best param found at split 1: l2reg = 0.0001                 with score 88.15
04/22 04:36:53 PM: Best param found at split 2: l2reg = 0.001                 with score 87.38
04/22 04:38:00 PM: Best param found at split 3: l2reg = 0.0001                 with score 87.99
04/22 04:39:09 PM: Best param found at split 4: l2reg = 1e-05                 with score 87.91
04/22 04:40:21 PM: Best param found at split 5: l2reg = 1e-05                 with score 88.03
04/22 04:40:25 PM: Building vocabulary for task SUBJ...
04/22 04:40:25 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:40:27 PM: Building unique tokens of vocab based on SUBJ..
04/22 04:40:27 PM: Building w2i and aligned embeddings for task SUBJ
04/22 04:40:27 PM: Generating sentence embeddings
04/22 04:40:27 PM: Generated sentence embeddings
04/22 04:40:27 PM: Training pytorch-MLP-nhid0-adam-bs64 with (inner) 5-fold cross-validation
04/22 04:41:44 PM: Best param found at split 1: l2reg = 0.0001                 with score 91.34
04/22 04:42:57 PM: Best param found at split 2: l2reg = 0.0001                 with score 91.36
04/22 04:44:15 PM: Best param found at split 3: l2reg = 1e-05                 with score 91.79
04/22 04:45:33 PM: Best param found at split 4: l2reg = 1e-05                 with score 91.66
04/22 04:46:51 PM: Best param found at split 5: l2reg = 1e-05                 with score 91.38
04/22 04:46:55 PM: Building vocabulary for task SST2...
04/22 04:46:55 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:46:57 PM: Building unique tokens of vocab based on SST2..
04/22 04:46:57 PM: Building w2i and aligned embeddings for task SST2
04/22 04:46:57 PM: Computing embedding for train
04/22 04:46:58 PM: Computed train embeddings
04/22 04:46:58 PM: Computing embedding for dev
04/22 04:46:58 PM: Computed dev embeddings
04/22 04:46:58 PM: Computing embedding for test
04/22 04:46:58 PM: Computed test embeddings
04/22 04:46:58 PM: Training pytorch-MLP-nhid0-adam-bs64 with standard validation..
04/22 04:48:37 PM: [('reg:1e-05', 79.13), ('reg:0.0001', 79.24), ('reg:0.001', 79.24), ('reg:0.01', 77.87)]
04/22 04:48:37 PM: Validation : best param found is reg = 0.0001 with score             79.24
04/22 04:48:37 PM: Evaluating...
04/22 04:49:02 PM: ***** Transfer task : TREC *****


04/22 04:49:02 PM: Building vocabulary for task TREC...
04/22 04:49:02 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:49:05 PM: Building unique tokens of vocab based on TREC..
04/22 04:49:05 PM: Building w2i and aligned embeddings for task TREC
04/22 04:49:05 PM: Computed train embeddings
04/22 04:49:05 PM: Computed test embeddings
04/22 04:49:05 PM: Training pytorch-MLP-nhid0-adam-bs64 with 5-fold cross-validation
04/22 04:50:15 PM: [('reg:1e-05', 73.86), ('reg:0.0001', 73.68), ('reg:0.001', 72.96), ('reg:0.01', 65.65)]
04/22 04:50:15 PM: Cross-validation : best param found is reg = 1e-05             with score 73.86
04/22 04:50:15 PM: Evaluating...
04/22 04:50:18 PM: ***** Transfer task : MRPC *****


04/22 04:50:18 PM: Building vocabulary for task MRPC...
04/22 04:50:18 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:50:20 PM: Building unique tokens of vocab based on MRPC..
04/22 04:50:20 PM: Building w2i and aligned embeddings for task MRPC
04/22 04:50:21 PM: Computing embedding for train
04/22 04:50:21 PM: Computed train embeddings
04/22 04:50:21 PM: Computing embedding for test
04/22 04:50:21 PM: Computed test embeddings
04/22 04:50:21 PM: Training pytorch-MLP-nhid0-adam-bs64 with 5-fold cross-validation
04/22 04:50:53 PM: [('reg:1e-05', 74.02), ('reg:0.0001', 73.87), ('reg:0.001', 73.21), ('reg:0.01', 71.74)]
04/22 04:50:53 PM: Cross-validation : best param found is reg = 1e-05             with score 74.02
04/22 04:50:53 PM: Evaluating...
04/22 04:50:55 PM: Building vocabulary for task SICKEntailment...
04/22 04:50:55 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:50:57 PM: Building unique tokens of vocab based on SICKEntailment..
04/22 04:50:57 PM: Building w2i and aligned embeddings for task SICKEntailment
04/22 04:50:57 PM: Computing embedding for train
04/22 04:50:57 PM: Computed train embeddings
04/22 04:50:57 PM: Computing embedding for dev
04/22 04:50:57 PM: Computed dev embeddings
04/22 04:50:57 PM: Computing embedding for test
04/22 04:50:58 PM: Computed test embeddings
04/22 04:50:58 PM: Training pytorch-MLP-nhid0-adam-bs64 with standard validation..
04/22 04:51:10 PM: [('reg:1e-05', 81.0), ('reg:0.0001', 81.0), ('reg:0.001', 79.4), ('reg:0.01', 71.0)]
04/22 04:51:10 PM: Validation : best param found is reg = 1e-05 with score             81.0
04/22 04:51:10 PM: Evaluating...
04/22 04:51:13 PM: Building vocabulary for task STS14...
04/22 04:51:13 PM: Loading vectors from .vector_cache/glove.840B.300d.txt.pt
04/22 04:51:15 PM: Building unique tokens of vocab based on STS14..
04/22 04:51:15 PM: Building w2i and aligned embeddings for task STS14
04/22 04:51:15 PM: Results on SentEval tasks: {'MR': {'devacc': 77.69, 'acc': 77.33, 'ndev': 10662, 'ntest': 10662}, 'CR': {'devacc': 79.48, 'acc': 77.88, 'ndev': 3775, 'ntest': 3775}, 'MPQA': {'devacc': 87.89, 'acc': 87.71, 'ndev': 10606, 'ntest': 10606}, 'SUBJ': {'devacc': 91.51, 'acc': 91.4, 'ndev': 10000, 'ntest': 10000}, 'SST2': {'devacc': 79.24, 'acc': 80.29, 'ndev': 872, 'ntest': 1821}, 'TREC': {'devacc': 73.86, 'acc': 83.0, 'ndev': 5452, 'ntest': 500}, 'MRPC': {'devacc': 74.02, 'acc': 72.87, 'f1': 81.38, 'ndev': 4076, 'ntest': 1725}, 'SICKEntailment': {'devacc': 81.0, 'acc': 78.49, 'ndev': 500, 'ntest': 4927}, 'STS14': {'deft-forum': {'pearson': PearsonRResult(statistic=0.30015698920392164, pvalue=8.028873135607622e-11), 'spearman': SignificanceResult(statistic=0.34719792930633214, pvalue=3.407223149511437e-14), 'nsamples': 450}, 'deft-news': {'pearson': PearsonRResult(statistic=0.6494705707929106, pvalue=2.501700382866815e-37), 'spearman': SignificanceResult(statistic=0.6456219780351091, pvalue=9.041047812255562e-37), 'nsamples': 300}, 'headlines': {'pearson': PearsonRResult(statistic=0.586720905706843, pvalue=1.437358864145757e-70), 'spearman': SignificanceResult(statistic=0.5510040795093948, pvalue=8.917978464050785e-61), 'nsamples': 750}, 'images': {'pearson': PearsonRResult(statistic=0.6240477685364034, pvalue=3.406366662482351e-82), 'spearman': SignificanceResult(statistic=0.6127334964976213, pvalue=1.662435217385402e-78), 'nsamples': 750}, 'OnWN': {'pearson': PearsonRResult(statistic=0.577094279673974, pvalue=8.24707804240312e-68), 'spearman': SignificanceResult(statistic=0.6434659871555309, pvalue=6.851316923329252e-89), 'nsamples': 750}, 'tweet-news': {'pearson': PearsonRResult(statistic=0.5384018263562398, pvalue=1.3526868744507565e-57), 'spearman': SignificanceResult(statistic=0.5379424869568135, pvalue=1.756348757209049e-57), 'nsamples': 750}, 'all': {'pearson': {'mean': 0.5459820567117154, 'wmean': 0.5532294404225955}, 'spearman': {'mean': 0.5563276595768003, 'wmean': 0.5623427197834406}}}}
04/22 04:51:15 PM: --------------------------------
04/22 04:51:15 PM: Results with tokenization: False on SenteEval
04/22 04:51:15 PM: Results with building new Vocab. based on SentEval
04/22 04:51:15 PM: Seed used: 1111
04/22 04:51:15 PM: Macro accuracy: 80.5863
04/22 04:51:15 PM: Micro score: 82.4852
04/22 04:51:15 PM: --------------------------------

JOB STATISTICS
==============
Job ID: 6012465
Cluster: snellius
User/Group: scur1398/scur1398
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 07:48:18 core-walltime
Job Wall-clock time: 00:26:01
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
