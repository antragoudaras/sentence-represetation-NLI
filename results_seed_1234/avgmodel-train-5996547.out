04/22 01:32:40 AM: Printing arguments : Namespace(seed=1234, batch_size=64, num_workers=4, lr=0.1, lr_divisor=5, num_epochs=25, encoder='baseline', checkpoint=None)
04/22 01:32:40 AM: Setting seed...
04/22 01:32:40 AM: Building/Loading the SNLI dataset...
04/22 01:32:40 AM: Vocab already exists. Loading from disk...
04/22 01:32:49 AM: Total number of parameters: 10707251
04/22 01:32:50 AM: Training the model...
Adjusting learning rate of group 0 to 1.0000e-01.
  0%|          | 0/25 [00:00<?, ?it/s]04/22 01:32:50 AM: Epoch: 1/25
04/22 01:32:51 AM: Epoch: 1/25, Batch: 501/8584, Train Loss: 0.9979
04/22 01:32:52 AM: Epoch: 1/25, Batch: 1001/8584, Train Loss: 0.9069
04/22 01:32:53 AM: Epoch: 1/25, Batch: 1501/8584, Train Loss: 0.7692
04/22 01:32:55 AM: Epoch: 1/25, Batch: 2001/8584, Train Loss: 0.9380
04/22 01:32:56 AM: Epoch: 1/25, Batch: 2501/8584, Train Loss: 0.7892
04/22 01:32:57 AM: Epoch: 1/25, Batch: 3001/8584, Train Loss: 0.8980
04/22 01:32:58 AM: Epoch: 1/25, Batch: 3501/8584, Train Loss: 0.9985
04/22 01:32:59 AM: Epoch: 1/25, Batch: 4001/8584, Train Loss: 0.9966
04/22 01:33:00 AM: Epoch: 1/25, Batch: 4501/8584, Train Loss: 0.8661
04/22 01:33:01 AM: Epoch: 1/25, Batch: 5001/8584, Train Loss: 0.7916
04/22 01:33:02 AM: Epoch: 1/25, Batch: 5501/8584, Train Loss: 0.8416
04/22 01:33:03 AM: Epoch: 1/25, Batch: 6001/8584, Train Loss: 0.7919
04/22 01:33:04 AM: Epoch: 1/25, Batch: 6501/8584, Train Loss: 0.7901
04/22 01:33:05 AM: Epoch: 1/25, Batch: 7001/8584, Train Loss: 1.0402
04/22 01:33:06 AM: Epoch: 1/25, Batch: 7501/8584, Train Loss: 0.9571
04/22 01:33:07 AM: Epoch: 1/25, Batch: 8001/8584, Train Loss: 0.8473
04/22 01:33:08 AM: Epoch: 1/25, Batch: 8501/8584, Train Loss: 0.9671
04/22 01:33:09 AM: Epoch: 1/25, Train Loss: 0.9025
04/22 01:33:09 AM: Batch: 1/154, Loss: 0.8740, Acc: 53.1250
04/22 01:33:09 AM: Batch: 11/154, Loss: 0.8005, Acc: 62.5000
04/22 01:33:09 AM: Batch: 21/154, Loss: 0.8784, Acc: 59.3750
04/22 01:33:09 AM: Batch: 31/154, Loss: 0.6493, Acc: 73.4375
04/22 01:33:09 AM: Batch: 41/154, Loss: 0.7851, Acc: 65.6250
04/22 01:33:09 AM: Batch: 51/154, Loss: 0.8990, Acc: 54.6875
04/22 01:33:09 AM: Batch: 61/154, Loss: 0.9142, Acc: 60.9375
04/22 01:33:09 AM: Batch: 71/154, Loss: 0.8761, Acc: 59.3750
04/22 01:33:09 AM: Batch: 81/154, Loss: 0.8517, Acc: 67.1875
04/22 01:33:09 AM: Batch: 91/154, Loss: 0.8141, Acc: 75.0000
04/22 01:33:09 AM: Batch: 101/154, Loss: 0.8001, Acc: 68.7500
04/22 01:33:09 AM: Batch: 111/154, Loss: 0.7239, Acc: 68.7500
04/22 01:33:09 AM: Batch: 121/154, Loss: 0.7469, Acc: 70.3125
04/22 01:33:09 AM: Batch: 131/154, Loss: 0.9655, Acc: 57.8125
04/22 01:33:09 AM: Batch: 141/154, Loss: 0.8301, Acc: 65.6250
04/22 01:33:09 AM: Batch: 151/154, Loss: 0.8257, Acc: 60.9375
04/22 01:33:09 AM: Epoch: 1/25, Val Loss: 0.8268, Val Acc: 63.7574
04/22 01:33:09 AM: Best model found with val acc.: 63.7574
  4%|▍         | 1/25 [00:19<07:48, 19.50s/it]04/22 01:33:09 AM: Epoch: 2/25
Adjusting learning rate of group 0 to 9.9000e-02.
04/22 01:33:11 AM: Epoch: 2/25, Batch: 501/8584, Train Loss: 0.8281
04/22 01:33:12 AM: Epoch: 2/25, Batch: 1001/8584, Train Loss: 0.8542
04/22 01:33:13 AM: Epoch: 2/25, Batch: 1501/8584, Train Loss: 0.6947
04/22 01:33:14 AM: Epoch: 2/25, Batch: 2001/8584, Train Loss: 0.8653
04/22 01:33:15 AM: Epoch: 2/25, Batch: 2501/8584, Train Loss: 0.7274
04/22 01:33:16 AM: Epoch: 2/25, Batch: 3001/8584, Train Loss: 0.7841
04/22 01:33:17 AM: Epoch: 2/25, Batch: 3501/8584, Train Loss: 0.9713
04/22 01:33:18 AM: Epoch: 2/25, Batch: 4001/8584, Train Loss: 0.9478
04/22 01:33:19 AM: Epoch: 2/25, Batch: 4501/8584, Train Loss: 0.8577
04/22 01:33:20 AM: Epoch: 2/25, Batch: 5001/8584, Train Loss: 0.7678
04/22 01:33:21 AM: Epoch: 2/25, Batch: 5501/8584, Train Loss: 0.7891
04/22 01:33:23 AM: Epoch: 2/25, Batch: 6001/8584, Train Loss: 0.7607
04/22 01:33:24 AM: Epoch: 2/25, Batch: 6501/8584, Train Loss: 0.7619
04/22 01:33:25 AM: Epoch: 2/25, Batch: 7001/8584, Train Loss: 0.9803
04/22 01:33:26 AM: Epoch: 2/25, Batch: 7501/8584, Train Loss: 0.9354
04/22 01:33:27 AM: Epoch: 2/25, Batch: 8001/8584, Train Loss: 0.8220
04/22 01:33:28 AM: Epoch: 2/25, Batch: 8501/8584, Train Loss: 0.9350
04/22 01:33:28 AM: Epoch: 2/25, Train Loss: 0.8492
04/22 01:33:28 AM: Batch: 1/154, Loss: 0.8388, Acc: 57.8125
04/22 01:33:28 AM: Batch: 11/154, Loss: 0.7785, Acc: 67.1875
04/22 01:33:28 AM: Batch: 21/154, Loss: 0.8656, Acc: 51.5625
04/22 01:33:28 AM: Batch: 31/154, Loss: 0.6353, Acc: 71.8750
04/22 01:33:29 AM: Batch: 41/154, Loss: 0.7611, Acc: 68.7500
04/22 01:33:29 AM: Batch: 51/154, Loss: 0.8719, Acc: 54.6875
04/22 01:33:29 AM: Batch: 61/154, Loss: 0.8936, Acc: 56.2500
04/22 01:33:29 AM: Batch: 71/154, Loss: 0.8842, Acc: 56.2500
04/22 01:33:29 AM: Batch: 81/154, Loss: 0.8467, Acc: 71.8750
04/22 01:33:29 AM: Batch: 91/154, Loss: 0.7968, Acc: 75.0000
04/22 01:33:29 AM: Batch: 101/154, Loss: 0.7910, Acc: 65.6250
04/22 01:33:29 AM: Batch: 111/154, Loss: 0.7032, Acc: 71.8750
04/22 01:33:29 AM: Batch: 121/154, Loss: 0.7391, Acc: 71.8750
04/22 01:33:29 AM: Batch: 131/154, Loss: 0.9639, Acc: 60.9375
04/22 01:33:29 AM: Batch: 141/154, Loss: 0.8136, Acc: 65.6250
04/22 01:33:29 AM: Batch: 151/154, Loss: 0.8138, Acc: 67.1875
04/22 01:33:29 AM: Epoch: 2/25, Val Loss: 0.8128, Val Acc: 64.3873
04/22 01:33:29 AM: Best model found with val acc.: 64.3873
  8%|▊         | 2/25 [00:39<07:29, 19.53s/it]04/22 01:33:29 AM: Epoch: 3/25
Adjusting learning rate of group 0 to 9.8010e-02.
04/22 01:33:30 AM: Epoch: 3/25, Batch: 501/8584, Train Loss: 0.8248
04/22 01:33:31 AM: Epoch: 3/25, Batch: 1001/8584, Train Loss: 0.8542
04/22 01:33:32 AM: Epoch: 3/25, Batch: 1501/8584, Train Loss: 0.6807
04/22 01:33:33 AM: Epoch: 3/25, Batch: 2001/8584, Train Loss: 0.8335
04/22 01:33:35 AM: Epoch: 3/25, Batch: 2501/8584, Train Loss: 0.7156
04/22 01:33:36 AM: Epoch: 3/25, Batch: 3001/8584, Train Loss: 0.7626
04/22 01:33:37 AM: Epoch: 3/25, Batch: 3501/8584, Train Loss: 0.9552
04/22 01:33:38 AM: Epoch: 3/25, Batch: 4001/8584, Train Loss: 0.9303
04/22 01:33:39 AM: Epoch: 3/25, Batch: 4501/8584, Train Loss: 0.8532
04/22 01:33:40 AM: Epoch: 3/25, Batch: 5001/8584, Train Loss: 0.7613
04/22 01:33:41 AM: Epoch: 3/25, Batch: 5501/8584, Train Loss: 0.7687
04/22 01:33:42 AM: Epoch: 3/25, Batch: 6001/8584, Train Loss: 0.7474
04/22 01:33:43 AM: Epoch: 3/25, Batch: 6501/8584, Train Loss: 0.7533
04/22 01:33:44 AM: Epoch: 3/25, Batch: 7001/8584, Train Loss: 0.9545
04/22 01:33:46 AM: Epoch: 3/25, Batch: 7501/8584, Train Loss: 0.9296
04/22 01:33:47 AM: Epoch: 3/25, Batch: 8001/8584, Train Loss: 0.8103
04/22 01:33:48 AM: Epoch: 3/25, Batch: 8501/8584, Train Loss: 0.9181
04/22 01:33:48 AM: Epoch: 3/25, Train Loss: 0.8356
04/22 01:33:48 AM: Batch: 1/154, Loss: 0.8203, Acc: 59.3750
04/22 01:33:48 AM: Batch: 11/154, Loss: 0.7679, Acc: 67.1875
04/22 01:33:48 AM: Batch: 21/154, Loss: 0.8584, Acc: 53.1250
04/22 01:33:48 AM: Batch: 31/154, Loss: 0.6300, Acc: 71.8750
04/22 01:33:48 AM: Batch: 41/154, Loss: 0.7495, Acc: 71.8750
04/22 01:33:48 AM: Batch: 51/154, Loss: 0.8591, Acc: 59.3750
04/22 01:33:48 AM: Batch: 61/154, Loss: 0.8834, Acc: 56.2500
04/22 01:33:48 AM: Batch: 71/154, Loss: 0.8885, Acc: 57.8125
04/22 01:33:48 AM: Batch: 81/154, Loss: 0.8449, Acc: 71.8750
04/22 01:33:48 AM: Batch: 91/154, Loss: 0.7903, Acc: 73.4375
04/22 01:33:48 AM: Batch: 101/154, Loss: 0.7866, Acc: 65.6250
04/22 01:33:48 AM: Batch: 111/154, Loss: 0.6974, Acc: 71.8750
04/22 01:33:48 AM: Batch: 121/154, Loss: 0.7393, Acc: 68.7500
04/22 01:33:48 AM: Batch: 131/154, Loss: 0.9610, Acc: 60.9375
04/22 01:33:48 AM: Batch: 141/154, Loss: 0.8056, Acc: 65.6250
04/22 01:33:48 AM: Batch: 151/154, Loss: 0.8100, Acc: 67.1875
04/22 01:33:48 AM: Epoch: 3/25, Val Loss: 0.8070, Val Acc: 64.7023
04/22 01:33:48 AM: Best model found with val acc.: 64.7023
 12%|█▏        | 3/25 [00:58<07:11, 19.60s/it]04/22 01:33:48 AM: Epoch: 4/25
Adjusting learning rate of group 0 to 9.7030e-02.
04/22 01:33:50 AM: Epoch: 4/25, Batch: 501/8584, Train Loss: 0.8227
04/22 01:33:51 AM: Epoch: 4/25, Batch: 1001/8584, Train Loss: 0.8547
04/22 01:33:52 AM: Epoch: 4/25, Batch: 1501/8584, Train Loss: 0.6765
04/22 01:33:53 AM: Epoch: 4/25, Batch: 2001/8584, Train Loss: 0.8190
04/22 01:33:54 AM: Epoch: 4/25, Batch: 2501/8584, Train Loss: 0.7105
04/22 01:33:55 AM: Epoch: 4/25, Batch: 3001/8584, Train Loss: 0.7534
04/22 01:33:56 AM: Epoch: 4/25, Batch: 3501/8584, Train Loss: 0.9439
04/22 01:33:57 AM: Epoch: 4/25, Batch: 4001/8584, Train Loss: 0.9213
04/22 01:33:59 AM: Epoch: 4/25, Batch: 4501/8584, Train Loss: 0.8510
04/22 01:34:00 AM: Epoch: 4/25, Batch: 5001/8584, Train Loss: 0.7570
04/22 01:34:01 AM: Epoch: 4/25, Batch: 5501/8584, Train Loss: 0.7576
04/22 01:34:02 AM: Epoch: 4/25, Batch: 6001/8584, Train Loss: 0.7414
04/22 01:34:03 AM: Epoch: 4/25, Batch: 6501/8584, Train Loss: 0.7487
04/22 01:34:04 AM: Epoch: 4/25, Batch: 7001/8584, Train Loss: 0.9400
04/22 01:34:05 AM: Epoch: 4/25, Batch: 7501/8584, Train Loss: 0.9272
04/22 01:34:06 AM: Epoch: 4/25, Batch: 8001/8584, Train Loss: 0.8023
04/22 01:34:07 AM: Epoch: 4/25, Batch: 8501/8584, Train Loss: 0.9074
04/22 01:34:07 AM: Epoch: 4/25, Train Loss: 0.8284
04/22 01:34:08 AM: Batch: 1/154, Loss: 0.8084, Acc: 57.8125
04/22 01:34:08 AM: Batch: 11/154, Loss: 0.7614, Acc: 68.7500
04/22 01:34:08 AM: Batch: 21/154, Loss: 0.8542, Acc: 54.6875
04/22 01:34:08 AM: Batch: 31/154, Loss: 0.6273, Acc: 73.4375
04/22 01:34:08 AM: Batch: 41/154, Loss: 0.7424, Acc: 70.3125
04/22 01:34:08 AM: Batch: 51/154, Loss: 0.8516, Acc: 62.5000
04/22 01:34:08 AM: Batch: 61/154, Loss: 0.8772, Acc: 53.1250
04/22 01:34:08 AM: Batch: 71/154, Loss: 0.8904, Acc: 57.8125
04/22 01:34:08 AM: Batch: 81/154, Loss: 0.8442, Acc: 68.7500
04/22 01:34:08 AM: Batch: 91/154, Loss: 0.7873, Acc: 73.4375
04/22 01:34:08 AM: Batch: 101/154, Loss: 0.7840, Acc: 65.6250
04/22 01:34:08 AM: Batch: 111/154, Loss: 0.6951, Acc: 70.3125
04/22 01:34:08 AM: Batch: 121/154, Loss: 0.7406, Acc: 68.7500
04/22 01:34:08 AM: Batch: 131/154, Loss: 0.9584, Acc: 60.9375
04/22 01:34:08 AM: Batch: 141/154, Loss: 0.8011, Acc: 68.7500
04/22 01:34:08 AM: Batch: 151/154, Loss: 0.8083, Acc: 65.6250
04/22 01:34:08 AM: Epoch: 4/25, Val Loss: 0.8039, Val Acc: 64.9360
04/22 01:34:08 AM: Best model found with val acc.: 64.9360
 16%|█▌        | 4/25 [01:18<06:50, 19.55s/it]04/22 01:34:08 AM: Epoch: 5/25
Adjusting learning rate of group 0 to 9.6060e-02.
04/22 01:34:09 AM: Epoch: 5/25, Batch: 501/8584, Train Loss: 0.8206
04/22 01:34:10 AM: Epoch: 5/25, Batch: 1001/8584, Train Loss: 0.8550
04/22 01:34:11 AM: Epoch: 5/25, Batch: 1501/8584, Train Loss: 0.6754
04/22 01:34:13 AM: Epoch: 5/25, Batch: 2001/8584, Train Loss: 0.8111
04/22 01:34:14 AM: Epoch: 5/25, Batch: 2501/8584, Train Loss: 0.7076
04/22 01:34:15 AM: Epoch: 5/25, Batch: 3001/8584, Train Loss: 0.7487
04/22 01:34:16 AM: Epoch: 5/25, Batch: 3501/8584, Train Loss: 0.9357
04/22 01:34:17 AM: Epoch: 5/25, Batch: 4001/8584, Train Loss: 0.9162
04/22 01:34:18 AM: Epoch: 5/25, Batch: 4501/8584, Train Loss: 0.8499
04/22 01:34:19 AM: Epoch: 5/25, Batch: 5001/8584, Train Loss: 0.7534
04/22 01:34:20 AM: Epoch: 5/25, Batch: 5501/8584, Train Loss: 0.7506
04/22 01:34:21 AM: Epoch: 5/25, Batch: 6001/8584, Train Loss: 0.7382
04/22 01:34:22 AM: Epoch: 5/25, Batch: 6501/8584, Train Loss: 0.7458
04/22 01:34:23 AM: Epoch: 5/25, Batch: 7001/8584, Train Loss: 0.9307
04/22 01:34:25 AM: Epoch: 5/25, Batch: 7501/8584, Train Loss: 0.9257
04/22 01:34:26 AM: Epoch: 5/25, Batch: 8001/8584, Train Loss: 0.7963
04/22 01:34:27 AM: Epoch: 5/25, Batch: 8501/8584, Train Loss: 0.9000
04/22 01:34:27 AM: Epoch: 5/25, Train Loss: 0.8239
04/22 01:34:27 AM: Batch: 1/154, Loss: 0.7999, Acc: 57.8125
04/22 01:34:27 AM: Batch: 11/154, Loss: 0.7571, Acc: 68.7500
04/22 01:34:27 AM: Batch: 21/154, Loss: 0.8516, Acc: 54.6875
04/22 01:34:27 AM: Batch: 31/154, Loss: 0.6256, Acc: 75.0000
04/22 01:34:27 AM: Batch: 41/154, Loss: 0.7374, Acc: 70.3125
04/22 01:34:27 AM: Batch: 51/154, Loss: 0.8467, Acc: 62.5000
04/22 01:34:27 AM: Batch: 61/154, Loss: 0.8728, Acc: 53.1250
04/22 01:34:27 AM: Batch: 71/154, Loss: 0.8911, Acc: 57.8125
04/22 01:34:27 AM: Batch: 81/154, Loss: 0.8437, Acc: 67.1875
04/22 01:34:27 AM: Batch: 91/154, Loss: 0.7856, Acc: 73.4375
04/22 01:34:27 AM: Batch: 101/154, Loss: 0.7825, Acc: 68.7500
04/22 01:34:27 AM: Batch: 111/154, Loss: 0.6937, Acc: 70.3125
04/22 01:34:27 AM: Batch: 121/154, Loss: 0.7419, Acc: 68.7500
04/22 01:34:27 AM: Batch: 131/154, Loss: 0.9562, Acc: 64.0625
04/22 01:34:27 AM: Batch: 141/154, Loss: 0.7982, Acc: 68.7500
04/22 01:34:27 AM: Batch: 151/154, Loss: 0.8070, Acc: 67.1875
04/22 01:34:27 AM: Epoch: 5/25, Val Loss: 0.8018, Val Acc: 65.1494
04/22 01:34:27 AM: Best model found with val acc.: 65.1494
 20%|██        | 5/25 [01:37<06:31, 19.56s/it]04/22 01:34:28 AM: Epoch: 6/25
Adjusting learning rate of group 0 to 9.5099e-02.
04/22 01:34:29 AM: Epoch: 6/25, Batch: 501/8584, Train Loss: 0.8190
04/22 01:34:30 AM: Epoch: 6/25, Batch: 1001/8584, Train Loss: 0.8552
04/22 01:34:31 AM: Epoch: 6/25, Batch: 1501/8584, Train Loss: 0.6756
04/22 01:34:32 AM: Epoch: 6/25, Batch: 2001/8584, Train Loss: 0.8063
04/22 01:34:33 AM: Epoch: 6/25, Batch: 2501/8584, Train Loss: 0.7054
04/22 01:34:34 AM: Epoch: 6/25, Batch: 3001/8584, Train Loss: 0.7462
04/22 01:34:35 AM: Epoch: 6/25, Batch: 3501/8584, Train Loss: 0.9297
04/22 01:34:36 AM: Epoch: 6/25, Batch: 4001/8584, Train Loss: 0.9130
04/22 01:34:37 AM: Epoch: 6/25, Batch: 4501/8584, Train Loss: 0.8493
04/22 01:34:39 AM: Epoch: 6/25, Batch: 5001/8584, Train Loss: 0.7504
04/22 01:34:40 AM: Epoch: 6/25, Batch: 5501/8584, Train Loss: 0.7458
04/22 01:34:41 AM: Epoch: 6/25, Batch: 6001/8584, Train Loss: 0.7363
04/22 01:34:42 AM: Epoch: 6/25, Batch: 6501/8584, Train Loss: 0.7438
04/22 01:34:43 AM: Epoch: 6/25, Batch: 7001/8584, Train Loss: 0.9242
04/22 01:34:44 AM: Epoch: 6/25, Batch: 7501/8584, Train Loss: 0.9246
04/22 01:34:45 AM: Epoch: 6/25, Batch: 8001/8584, Train Loss: 0.7915
04/22 01:34:46 AM: Epoch: 6/25, Batch: 8501/8584, Train Loss: 0.8945
04/22 01:34:46 AM: Epoch: 6/25, Train Loss: 0.8207
04/22 01:34:46 AM: Batch: 1/154, Loss: 0.7933, Acc: 59.3750
04/22 01:34:47 AM: Batch: 11/154, Loss: 0.7539, Acc: 65.6250
04/22 01:34:47 AM: Batch: 21/154, Loss: 0.8497, Acc: 56.2500
04/22 01:34:47 AM: Batch: 31/154, Loss: 0.6243, Acc: 75.0000
04/22 01:34:47 AM: Batch: 41/154, Loss: 0.7337, Acc: 70.3125
04/22 01:34:47 AM: Batch: 51/154, Loss: 0.8431, Acc: 62.5000
04/22 01:34:47 AM: Batch: 61/154, Loss: 0.8693, Acc: 53.1250
04/22 01:34:47 AM: Batch: 71/154, Loss: 0.8913, Acc: 57.8125
04/22 01:34:47 AM: Batch: 81/154, Loss: 0.8434, Acc: 67.1875
04/22 01:34:47 AM: Batch: 91/154, Loss: 0.7847, Acc: 73.4375
04/22 01:34:47 AM: Batch: 101/154, Loss: 0.7816, Acc: 68.7500
04/22 01:34:47 AM: Batch: 111/154, Loss: 0.6925, Acc: 70.3125
04/22 01:34:47 AM: Batch: 121/154, Loss: 0.7428, Acc: 68.7500
04/22 01:34:47 AM: Batch: 131/154, Loss: 0.9543, Acc: 62.5000
04/22 01:34:47 AM: Batch: 141/154, Loss: 0.7961, Acc: 68.7500
04/22 01:34:47 AM: Batch: 151/154, Loss: 0.8059, Acc: 67.1875
04/22 01:34:47 AM: Epoch: 6/25, Val Loss: 0.8002, Val Acc: 65.1900
04/22 01:34:47 AM: Best model found with val acc.: 65.1900
 24%|██▍       | 6/25 [01:57<06:10, 19.50s/it]04/22 01:34:47 AM: Epoch: 7/25
Adjusting learning rate of group 0 to 9.4148e-02.
04/22 01:34:48 AM: Epoch: 7/25, Batch: 501/8584, Train Loss: 0.8178
04/22 01:34:49 AM: Epoch: 7/25, Batch: 1001/8584, Train Loss: 0.8554
04/22 01:34:50 AM: Epoch: 7/25, Batch: 1501/8584, Train Loss: 0.6764
04/22 01:34:52 AM: Epoch: 7/25, Batch: 2001/8584, Train Loss: 0.8033
04/22 01:34:53 AM: Epoch: 7/25, Batch: 2501/8584, Train Loss: 0.7037
04/22 01:34:54 AM: Epoch: 7/25, Batch: 3001/8584, Train Loss: 0.7447
04/22 01:34:55 AM: Epoch: 7/25, Batch: 3501/8584, Train Loss: 0.9251
04/22 01:34:56 AM: Epoch: 7/25, Batch: 4001/8584, Train Loss: 0.9109
04/22 01:34:57 AM: Epoch: 7/25, Batch: 4501/8584, Train Loss: 0.8489
04/22 01:34:58 AM: Epoch: 7/25, Batch: 5001/8584, Train Loss: 0.7477
04/22 01:34:59 AM: Epoch: 7/25, Batch: 5501/8584, Train Loss: 0.7422
04/22 01:35:00 AM: Epoch: 7/25, Batch: 6001/8584, Train Loss: 0.7351
04/22 01:35:01 AM: Epoch: 7/25, Batch: 6501/8584, Train Loss: 0.7425
04/22 01:35:02 AM: Epoch: 7/25, Batch: 7001/8584, Train Loss: 0.9193
04/22 01:35:04 AM: Epoch: 7/25, Batch: 7501/8584, Train Loss: 0.9237
04/22 01:35:05 AM: Epoch: 7/25, Batch: 8001/8584, Train Loss: 0.7877
04/22 01:35:06 AM: Epoch: 7/25, Batch: 8501/8584, Train Loss: 0.8904
04/22 01:35:06 AM: Epoch: 7/25, Train Loss: 0.8183
04/22 01:35:06 AM: Batch: 1/154, Loss: 0.7881, Acc: 59.3750
04/22 01:35:06 AM: Batch: 11/154, Loss: 0.7513, Acc: 65.6250
04/22 01:35:06 AM: Batch: 21/154, Loss: 0.8482, Acc: 56.2500
04/22 01:35:06 AM: Batch: 31/154, Loss: 0.6232, Acc: 76.5625
04/22 01:35:06 AM: Batch: 41/154, Loss: 0.7308, Acc: 70.3125
04/22 01:35:06 AM: Batch: 51/154, Loss: 0.8404, Acc: 64.0625
04/22 01:35:06 AM: Batch: 61/154, Loss: 0.8665, Acc: 53.1250
04/22 01:35:06 AM: Batch: 71/154, Loss: 0.8911, Acc: 57.8125
04/22 01:35:06 AM: Batch: 81/154, Loss: 0.8430, Acc: 67.1875
04/22 01:35:06 AM: Batch: 91/154, Loss: 0.7840, Acc: 71.8750
04/22 01:35:06 AM: Batch: 101/154, Loss: 0.7810, Acc: 68.7500
04/22 01:35:06 AM: Batch: 111/154, Loss: 0.6914, Acc: 71.8750
04/22 01:35:06 AM: Batch: 121/154, Loss: 0.7434, Acc: 68.7500
04/22 01:35:06 AM: Batch: 131/154, Loss: 0.9527, Acc: 62.5000
04/22 01:35:06 AM: Batch: 141/154, Loss: 0.7946, Acc: 68.7500
04/22 01:35:06 AM: Batch: 151/154, Loss: 0.8048, Acc: 67.1875
04/22 01:35:07 AM: Epoch: 7/25, Val Loss: 0.7990, Val Acc: 65.3424
04/22 01:35:07 AM: Best model found with val acc.: 65.3424
 28%|██▊       | 7/25 [02:16<05:51, 19.55s/it]04/22 01:35:07 AM: Epoch: 8/25
Adjusting learning rate of group 0 to 9.3207e-02.
04/22 01:35:08 AM: Epoch: 8/25, Batch: 501/8584, Train Loss: 0.8168
04/22 01:35:09 AM: Epoch: 8/25, Batch: 1001/8584, Train Loss: 0.8556
04/22 01:35:10 AM: Epoch: 8/25, Batch: 1501/8584, Train Loss: 0.6774
04/22 01:35:11 AM: Epoch: 8/25, Batch: 2001/8584, Train Loss: 0.8013
04/22 01:35:12 AM: Epoch: 8/25, Batch: 2501/8584, Train Loss: 0.7022
04/22 01:35:13 AM: Epoch: 8/25, Batch: 3001/8584, Train Loss: 0.7438
04/22 01:35:14 AM: Epoch: 8/25, Batch: 3501/8584, Train Loss: 0.9214
04/22 01:35:16 AM: Epoch: 8/25, Batch: 4001/8584, Train Loss: 0.9094
04/22 01:35:17 AM: Epoch: 8/25, Batch: 4501/8584, Train Loss: 0.8485
04/22 01:35:18 AM: Epoch: 8/25, Batch: 5001/8584, Train Loss: 0.7455
04/22 01:35:19 AM: Epoch: 8/25, Batch: 5501/8584, Train Loss: 0.7395
04/22 01:35:20 AM: Epoch: 8/25, Batch: 6001/8584, Train Loss: 0.7341
04/22 01:35:21 AM: Epoch: 8/25, Batch: 6501/8584, Train Loss: 0.7416
04/22 01:35:22 AM: Epoch: 8/25, Batch: 7001/8584, Train Loss: 0.9155
04/22 01:35:23 AM: Epoch: 8/25, Batch: 7501/8584, Train Loss: 0.9230
04/22 01:35:24 AM: Epoch: 8/25, Batch: 8001/8584, Train Loss: 0.7845
04/22 01:35:25 AM: Epoch: 8/25, Batch: 8501/8584, Train Loss: 0.8872
04/22 01:35:26 AM: Epoch: 8/25, Train Loss: 0.8164
04/22 01:35:26 AM: Batch: 1/154, Loss: 0.7838, Acc: 60.9375
04/22 01:35:26 AM: Batch: 11/154, Loss: 0.7492, Acc: 65.6250
04/22 01:35:26 AM: Batch: 21/154, Loss: 0.8469, Acc: 57.8125
04/22 01:35:26 AM: Batch: 31/154, Loss: 0.6221, Acc: 76.5625
04/22 01:35:26 AM: Batch: 41/154, Loss: 0.7284, Acc: 71.8750
04/22 01:35:26 AM: Batch: 51/154, Loss: 0.8381, Acc: 64.0625
04/22 01:35:26 AM: Batch: 61/154, Loss: 0.8640, Acc: 53.1250
04/22 01:35:26 AM: Batch: 71/154, Loss: 0.8908, Acc: 59.3750
04/22 01:35:26 AM: Batch: 81/154, Loss: 0.8427, Acc: 67.1875
04/22 01:35:26 AM: Batch: 91/154, Loss: 0.7834, Acc: 70.3125
04/22 01:35:26 AM: Batch: 101/154, Loss: 0.7806, Acc: 68.7500
04/22 01:35:26 AM: Batch: 111/154, Loss: 0.6903, Acc: 71.8750
04/22 01:35:26 AM: Batch: 121/154, Loss: 0.7437, Acc: 68.7500
04/22 01:35:26 AM: Batch: 131/154, Loss: 0.9512, Acc: 64.0625
04/22 01:35:26 AM: Batch: 141/154, Loss: 0.7934, Acc: 68.7500
04/22 01:35:26 AM: Batch: 151/154, Loss: 0.8037, Acc: 67.1875
04/22 01:35:26 AM: Epoch: 8/25, Val Loss: 0.7979, Val Acc: 65.4034
04/22 01:35:26 AM: Best model found with val acc.: 65.4034
 32%|███▏      | 8/25 [02:36<05:32, 19.55s/it]04/22 01:35:26 AM: Epoch: 9/25
Adjusting learning rate of group 0 to 9.2274e-02.
04/22 01:35:27 AM: Epoch: 9/25, Batch: 501/8584, Train Loss: 0.8160
04/22 01:35:29 AM: Epoch: 9/25, Batch: 1001/8584, Train Loss: 0.8558
04/22 01:35:30 AM: Epoch: 9/25, Batch: 1501/8584, Train Loss: 0.6784
04/22 01:35:31 AM: Epoch: 9/25, Batch: 2001/8584, Train Loss: 0.7999
04/22 01:35:32 AM: Epoch: 9/25, Batch: 2501/8584, Train Loss: 0.7008
04/22 01:35:33 AM: Epoch: 9/25, Batch: 3001/8584, Train Loss: 0.7432
04/22 01:35:34 AM: Epoch: 9/25, Batch: 3501/8584, Train Loss: 0.9183
04/22 01:35:35 AM: Epoch: 9/25, Batch: 4001/8584, Train Loss: 0.9083
04/22 01:35:36 AM: Epoch: 9/25, Batch: 4501/8584, Train Loss: 0.8482
04/22 01:35:37 AM: Epoch: 9/25, Batch: 5001/8584, Train Loss: 0.7435
04/22 01:35:38 AM: Epoch: 9/25, Batch: 5501/8584, Train Loss: 0.7373
04/22 01:35:40 AM: Epoch: 9/25, Batch: 6001/8584, Train Loss: 0.7333
04/22 01:35:41 AM: Epoch: 9/25, Batch: 6501/8584, Train Loss: 0.7409
04/22 01:35:42 AM: Epoch: 9/25, Batch: 7001/8584, Train Loss: 0.9125
04/22 01:35:43 AM: Epoch: 9/25, Batch: 7501/8584, Train Loss: 0.9223
04/22 01:35:44 AM: Epoch: 9/25, Batch: 8001/8584, Train Loss: 0.7819
04/22 01:35:45 AM: Epoch: 9/25, Batch: 8501/8584, Train Loss: 0.8845
04/22 01:35:45 AM: Epoch: 9/25, Train Loss: 0.8149
04/22 01:35:45 AM: Batch: 1/154, Loss: 0.7802, Acc: 64.0625
04/22 01:35:45 AM: Batch: 11/154, Loss: 0.7474, Acc: 65.6250
04/22 01:35:45 AM: Batch: 21/154, Loss: 0.8459, Acc: 57.8125
04/22 01:35:45 AM: Batch: 31/154, Loss: 0.6212, Acc: 76.5625
04/22 01:35:45 AM: Batch: 41/154, Loss: 0.7265, Acc: 71.8750
04/22 01:35:45 AM: Batch: 51/154, Loss: 0.8362, Acc: 64.0625
04/22 01:35:45 AM: Batch: 61/154, Loss: 0.8618, Acc: 54.6875
04/22 01:35:45 AM: Batch: 71/154, Loss: 0.8904, Acc: 59.3750
04/22 01:35:46 AM: Batch: 81/154, Loss: 0.8424, Acc: 67.1875
04/22 01:35:46 AM: Batch: 91/154, Loss: 0.7829, Acc: 70.3125
04/22 01:35:46 AM: Batch: 101/154, Loss: 0.7805, Acc: 67.1875
04/22 01:35:46 AM: Batch: 111/154, Loss: 0.6893, Acc: 71.8750
04/22 01:35:46 AM: Batch: 121/154, Loss: 0.7437, Acc: 68.7500
04/22 01:35:46 AM: Batch: 131/154, Loss: 0.9498, Acc: 62.5000
04/22 01:35:46 AM: Batch: 141/154, Loss: 0.7924, Acc: 68.7500
04/22 01:35:46 AM: Batch: 151/154, Loss: 0.8026, Acc: 67.1875
04/22 01:35:46 AM: Epoch: 9/25, Val Loss: 0.7970, Val Acc: 65.4440
04/22 01:35:46 AM: Best model found with val acc.: 65.4440
 36%|███▌      | 9/25 [02:55<05:13, 19.57s/it]04/22 01:35:46 AM: Epoch: 10/25
Adjusting learning rate of group 0 to 9.1352e-02.
04/22 01:35:47 AM: Epoch: 10/25, Batch: 501/8584, Train Loss: 0.8152
04/22 01:35:48 AM: Epoch: 10/25, Batch: 1001/8584, Train Loss: 0.8559
04/22 01:35:49 AM: Epoch: 10/25, Batch: 1501/8584, Train Loss: 0.6795
04/22 01:35:50 AM: Epoch: 10/25, Batch: 2001/8584, Train Loss: 0.7990
04/22 01:35:51 AM: Epoch: 10/25, Batch: 2501/8584, Train Loss: 0.6995
04/22 01:35:52 AM: Epoch: 10/25, Batch: 3001/8584, Train Loss: 0.7427
04/22 01:35:54 AM: Epoch: 10/25, Batch: 3501/8584, Train Loss: 0.9157
04/22 01:35:55 AM: Epoch: 10/25, Batch: 4001/8584, Train Loss: 0.9075
04/22 01:35:56 AM: Epoch: 10/25, Batch: 4501/8584, Train Loss: 0.8478
04/22 01:35:57 AM: Epoch: 10/25, Batch: 5001/8584, Train Loss: 0.7418
04/22 01:35:58 AM: Epoch: 10/25, Batch: 5501/8584, Train Loss: 0.7355
04/22 01:35:59 AM: Epoch: 10/25, Batch: 6001/8584, Train Loss: 0.7326
04/22 01:36:00 AM: Epoch: 10/25, Batch: 6501/8584, Train Loss: 0.7405
04/22 01:36:01 AM: Epoch: 10/25, Batch: 7001/8584, Train Loss: 0.9099
04/22 01:36:02 AM: Epoch: 10/25, Batch: 7501/8584, Train Loss: 0.9218
04/22 01:36:03 AM: Epoch: 10/25, Batch: 8001/8584, Train Loss: 0.7797
04/22 01:36:05 AM: Epoch: 10/25, Batch: 8501/8584, Train Loss: 0.8823
04/22 01:36:05 AM: Epoch: 10/25, Train Loss: 0.8136
04/22 01:36:05 AM: Batch: 1/154, Loss: 0.7772, Acc: 64.0625
04/22 01:36:05 AM: Batch: 11/154, Loss: 0.7459, Acc: 65.6250
04/22 01:36:05 AM: Batch: 21/154, Loss: 0.8449, Acc: 59.3750
04/22 01:36:05 AM: Batch: 31/154, Loss: 0.6204, Acc: 76.5625
04/22 01:36:05 AM: Batch: 41/154, Loss: 0.7249, Acc: 71.8750
04/22 01:36:05 AM: Batch: 51/154, Loss: 0.8345, Acc: 64.0625
04/22 01:36:05 AM: Batch: 61/154, Loss: 0.8599, Acc: 54.6875
04/22 01:36:05 AM: Batch: 71/154, Loss: 0.8899, Acc: 59.3750
04/22 01:36:05 AM: Batch: 81/154, Loss: 0.8420, Acc: 67.1875
04/22 01:36:05 AM: Batch: 91/154, Loss: 0.7825, Acc: 68.7500
04/22 01:36:05 AM: Batch: 101/154, Loss: 0.7804, Acc: 67.1875
04/22 01:36:05 AM: Batch: 111/154, Loss: 0.6884, Acc: 71.8750
04/22 01:36:05 AM: Batch: 121/154, Loss: 0.7436, Acc: 68.7500
04/22 01:36:05 AM: Batch: 131/154, Loss: 0.9486, Acc: 62.5000
04/22 01:36:05 AM: Batch: 141/154, Loss: 0.7916, Acc: 68.7500
04/22 01:36:05 AM: Batch: 151/154, Loss: 0.8015, Acc: 67.1875
04/22 01:36:05 AM: Epoch: 10/25, Val Loss: 0.7961, Val Acc: 65.5050
04/22 01:36:05 AM: Best model found with val acc.: 65.5050
 40%|████      | 10/25 [03:15<04:53, 19.59s/it]04/22 01:36:05 AM: Epoch: 11/25
Adjusting learning rate of group 0 to 9.0438e-02.
04/22 01:36:07 AM: Epoch: 11/25, Batch: 501/8584, Train Loss: 0.8145
04/22 01:36:08 AM: Epoch: 11/25, Batch: 1001/8584, Train Loss: 0.8561
04/22 01:36:09 AM: Epoch: 11/25, Batch: 1501/8584, Train Loss: 0.6805
04/22 01:36:10 AM: Epoch: 11/25, Batch: 2001/8584, Train Loss: 0.7985
04/22 01:36:11 AM: Epoch: 11/25, Batch: 2501/8584, Train Loss: 0.6984
04/22 01:36:12 AM: Epoch: 11/25, Batch: 3001/8584, Train Loss: 0.7423
04/22 01:36:13 AM: Epoch: 11/25, Batch: 3501/8584, Train Loss: 0.9134
04/22 01:36:14 AM: Epoch: 11/25, Batch: 4001/8584, Train Loss: 0.9068
04/22 01:36:15 AM: Epoch: 11/25, Batch: 4501/8584, Train Loss: 0.8475
04/22 01:36:16 AM: Epoch: 11/25, Batch: 5001/8584, Train Loss: 0.7404
04/22 01:36:18 AM: Epoch: 11/25, Batch: 5501/8584, Train Loss: 0.7339
04/22 01:36:19 AM: Epoch: 11/25, Batch: 6001/8584, Train Loss: 0.7319
04/22 01:36:20 AM: Epoch: 11/25, Batch: 6501/8584, Train Loss: 0.7402
04/22 01:36:21 AM: Epoch: 11/25, Batch: 7001/8584, Train Loss: 0.9078
04/22 01:36:22 AM: Epoch: 11/25, Batch: 7501/8584, Train Loss: 0.9213
04/22 01:36:23 AM: Epoch: 11/25, Batch: 8001/8584, Train Loss: 0.7778
04/22 01:36:24 AM: Epoch: 11/25, Batch: 8501/8584, Train Loss: 0.8805
04/22 01:36:24 AM: Epoch: 11/25, Train Loss: 0.8126
04/22 01:36:25 AM: Batch: 1/154, Loss: 0.7746, Acc: 64.0625
04/22 01:36:25 AM: Batch: 11/154, Loss: 0.7445, Acc: 64.0625
04/22 01:36:25 AM: Batch: 21/154, Loss: 0.8441, Acc: 59.3750
04/22 01:36:25 AM: Batch: 31/154, Loss: 0.6196, Acc: 76.5625
04/22 01:36:25 AM: Batch: 41/154, Loss: 0.7235, Acc: 71.8750
04/22 01:36:25 AM: Batch: 51/154, Loss: 0.8331, Acc: 60.9375
04/22 01:36:25 AM: Batch: 61/154, Loss: 0.8582, Acc: 54.6875
04/22 01:36:25 AM: Batch: 71/154, Loss: 0.8895, Acc: 59.3750
04/22 01:36:25 AM: Batch: 81/154, Loss: 0.8416, Acc: 67.1875
04/22 01:36:25 AM: Batch: 91/154, Loss: 0.7820, Acc: 68.7500
04/22 01:36:25 AM: Batch: 101/154, Loss: 0.7804, Acc: 67.1875
04/22 01:36:25 AM: Batch: 111/154, Loss: 0.6875, Acc: 73.4375
04/22 01:36:25 AM: Batch: 121/154, Loss: 0.7433, Acc: 68.7500
04/22 01:36:25 AM: Batch: 131/154, Loss: 0.9474, Acc: 62.5000
04/22 01:36:25 AM: Batch: 141/154, Loss: 0.7909, Acc: 68.7500
04/22 01:36:25 AM: Batch: 151/154, Loss: 0.8004, Acc: 67.1875
04/22 01:36:25 AM: Epoch: 11/25, Val Loss: 0.7954, Val Acc: 65.5151
04/22 01:36:25 AM: Best model found with val acc.: 65.5151
 44%|████▍     | 11/25 [03:35<04:33, 19.57s/it]04/22 01:36:25 AM: Epoch: 12/25
Adjusting learning rate of group 0 to 8.9534e-02.
04/22 01:36:26 AM: Epoch: 12/25, Batch: 501/8584, Train Loss: 0.8138
04/22 01:36:27 AM: Epoch: 12/25, Batch: 1001/8584, Train Loss: 0.8562
04/22 01:36:28 AM: Epoch: 12/25, Batch: 1501/8584, Train Loss: 0.6815
04/22 01:36:29 AM: Epoch: 12/25, Batch: 2001/8584, Train Loss: 0.7982
04/22 01:36:31 AM: Epoch: 12/25, Batch: 2501/8584, Train Loss: 0.6973
04/22 01:36:32 AM: Epoch: 12/25, Batch: 3001/8584, Train Loss: 0.7419
04/22 01:36:33 AM: Epoch: 12/25, Batch: 3501/8584, Train Loss: 0.9115
04/22 01:36:34 AM: Epoch: 12/25, Batch: 4001/8584, Train Loss: 0.9063
04/22 01:36:35 AM: Epoch: 12/25, Batch: 4501/8584, Train Loss: 0.8472
04/22 01:36:36 AM: Epoch: 12/25, Batch: 5001/8584, Train Loss: 0.7391
04/22 01:36:37 AM: Epoch: 12/25, Batch: 5501/8584, Train Loss: 0.7326
04/22 01:36:38 AM: Epoch: 12/25, Batch: 6001/8584, Train Loss: 0.7312
04/22 01:36:39 AM: Epoch: 12/25, Batch: 6501/8584, Train Loss: 0.7401
04/22 01:36:40 AM: Epoch: 12/25, Batch: 7001/8584, Train Loss: 0.9060
04/22 01:36:41 AM: Epoch: 12/25, Batch: 7501/8584, Train Loss: 0.9208
04/22 01:36:43 AM: Epoch: 12/25, Batch: 8001/8584, Train Loss: 0.7761
04/22 01:36:44 AM: Epoch: 12/25, Batch: 8501/8584, Train Loss: 0.8789
04/22 01:36:44 AM: Epoch: 12/25, Train Loss: 0.8116
04/22 01:36:44 AM: Batch: 1/154, Loss: 0.7723, Acc: 64.0625
04/22 01:36:44 AM: Batch: 11/154, Loss: 0.7433, Acc: 64.0625
04/22 01:36:44 AM: Batch: 21/154, Loss: 0.8433, Acc: 59.3750
04/22 01:36:44 AM: Batch: 31/154, Loss: 0.6188, Acc: 78.1250
04/22 01:36:44 AM: Batch: 41/154, Loss: 0.7222, Acc: 73.4375
04/22 01:36:44 AM: Batch: 51/154, Loss: 0.8318, Acc: 60.9375
04/22 01:36:44 AM: Batch: 61/154, Loss: 0.8566, Acc: 54.6875
04/22 01:36:44 AM: Batch: 71/154, Loss: 0.8890, Acc: 59.3750
04/22 01:36:44 AM: Batch: 81/154, Loss: 0.8412, Acc: 67.1875
04/22 01:36:44 AM: Batch: 91/154, Loss: 0.7816, Acc: 70.3125
04/22 01:36:44 AM: Batch: 101/154, Loss: 0.7805, Acc: 67.1875
04/22 01:36:44 AM: Batch: 111/154, Loss: 0.6866, Acc: 73.4375
04/22 01:36:44 AM: Batch: 121/154, Loss: 0.7429, Acc: 68.7500
04/22 01:36:44 AM: Batch: 131/154, Loss: 0.9464, Acc: 62.5000
04/22 01:36:44 AM: Batch: 141/154, Loss: 0.7903, Acc: 68.7500
04/22 01:36:44 AM: Batch: 151/154, Loss: 0.7993, Acc: 67.1875
04/22 01:36:44 AM: Epoch: 12/25, Val Loss: 0.7946, Val Acc: 65.5863
04/22 01:36:44 AM: Best model found with val acc.: 65.5863
 48%|████▊     | 12/25 [03:54<04:14, 19.55s/it]04/22 01:36:44 AM: Epoch: 13/25
Adjusting learning rate of group 0 to 8.8638e-02.
04/22 01:36:46 AM: Epoch: 13/25, Batch: 501/8584, Train Loss: 0.8132
04/22 01:36:47 AM: Epoch: 13/25, Batch: 1001/8584, Train Loss: 0.8563
04/22 01:36:48 AM: Epoch: 13/25, Batch: 1501/8584, Train Loss: 0.6823
04/22 01:36:49 AM: Epoch: 13/25, Batch: 2001/8584, Train Loss: 0.7981
04/22 01:36:50 AM: Epoch: 13/25, Batch: 2501/8584, Train Loss: 0.6963
04/22 01:36:51 AM: Epoch: 13/25, Batch: 3001/8584, Train Loss: 0.7415
04/22 01:36:52 AM: Epoch: 13/25, Batch: 3501/8584, Train Loss: 0.9097
04/22 01:36:53 AM: Epoch: 13/25, Batch: 4001/8584, Train Loss: 0.9058
04/22 01:36:55 AM: Epoch: 13/25, Batch: 4501/8584, Train Loss: 0.8469
04/22 01:36:56 AM: Epoch: 13/25, Batch: 5001/8584, Train Loss: 0.7379
04/22 01:36:57 AM: Epoch: 13/25, Batch: 5501/8584, Train Loss: 0.7314
04/22 01:36:58 AM: Epoch: 13/25, Batch: 6001/8584, Train Loss: 0.7305
04/22 01:36:59 AM: Epoch: 13/25, Batch: 6501/8584, Train Loss: 0.7400
04/22 01:37:00 AM: Epoch: 13/25, Batch: 7001/8584, Train Loss: 0.9044
04/22 01:37:01 AM: Epoch: 13/25, Batch: 7501/8584, Train Loss: 0.9205
04/22 01:37:02 AM: Epoch: 13/25, Batch: 8001/8584, Train Loss: 0.7747
04/22 01:37:03 AM: Epoch: 13/25, Batch: 8501/8584, Train Loss: 0.8775
04/22 01:37:03 AM: Epoch: 13/25, Train Loss: 0.8108
04/22 01:37:04 AM: Batch: 1/154, Loss: 0.7703, Acc: 62.5000
04/22 01:37:04 AM: Batch: 11/154, Loss: 0.7422, Acc: 64.0625
04/22 01:37:04 AM: Batch: 21/154, Loss: 0.8426, Acc: 59.3750
04/22 01:37:04 AM: Batch: 31/154, Loss: 0.6182, Acc: 78.1250
04/22 01:37:04 AM: Batch: 41/154, Loss: 0.7212, Acc: 75.0000
04/22 01:37:04 AM: Batch: 51/154, Loss: 0.8306, Acc: 60.9375
04/22 01:37:04 AM: Batch: 61/154, Loss: 0.8552, Acc: 54.6875
04/22 01:37:04 AM: Batch: 71/154, Loss: 0.8886, Acc: 57.8125
04/22 01:37:04 AM: Batch: 81/154, Loss: 0.8408, Acc: 67.1875
04/22 01:37:04 AM: Batch: 91/154, Loss: 0.7811, Acc: 67.1875
04/22 01:37:04 AM: Batch: 101/154, Loss: 0.7807, Acc: 67.1875
04/22 01:37:04 AM: Batch: 111/154, Loss: 0.6858, Acc: 73.4375
04/22 01:37:04 AM: Batch: 121/154, Loss: 0.7425, Acc: 68.7500
04/22 01:37:04 AM: Batch: 131/154, Loss: 0.9454, Acc: 62.5000
04/22 01:37:04 AM: Batch: 141/154, Loss: 0.7898, Acc: 70.3125
04/22 01:37:04 AM: Batch: 151/154, Loss: 0.7983, Acc: 67.1875
04/22 01:37:04 AM: Epoch: 13/25, Val Loss: 0.7940, Val Acc: 65.6167
04/22 01:37:04 AM: Best model found with val acc.: 65.6167
 52%|█████▏    | 13/25 [04:14<03:55, 19.58s/it]04/22 01:37:04 AM: Epoch: 14/25
Adjusting learning rate of group 0 to 8.7752e-02.
04/22 01:37:05 AM: Epoch: 14/25, Batch: 501/8584, Train Loss: 0.8126
04/22 01:37:06 AM: Epoch: 14/25, Batch: 1001/8584, Train Loss: 0.8564
04/22 01:37:08 AM: Epoch: 14/25, Batch: 1501/8584, Train Loss: 0.6831
04/22 01:37:09 AM: Epoch: 14/25, Batch: 2001/8584, Train Loss: 0.7981
04/22 01:37:10 AM: Epoch: 14/25, Batch: 2501/8584, Train Loss: 0.6954
04/22 01:37:11 AM: Epoch: 14/25, Batch: 3001/8584, Train Loss: 0.7410
04/22 01:37:12 AM: Epoch: 14/25, Batch: 3501/8584, Train Loss: 0.9081
04/22 01:37:13 AM: Epoch: 14/25, Batch: 4001/8584, Train Loss: 0.9054
04/22 01:37:14 AM: Epoch: 14/25, Batch: 4501/8584, Train Loss: 0.8466
04/22 01:37:15 AM: Epoch: 14/25, Batch: 5001/8584, Train Loss: 0.7369
04/22 01:37:16 AM: Epoch: 14/25, Batch: 5501/8584, Train Loss: 0.7303
04/22 01:37:17 AM: Epoch: 14/25, Batch: 6001/8584, Train Loss: 0.7299
04/22 01:37:18 AM: Epoch: 14/25, Batch: 6501/8584, Train Loss: 0.7401
04/22 01:37:19 AM: Epoch: 14/25, Batch: 7001/8584, Train Loss: 0.9029
04/22 01:37:21 AM: Epoch: 14/25, Batch: 7501/8584, Train Loss: 0.9201
04/22 01:37:22 AM: Epoch: 14/25, Batch: 8001/8584, Train Loss: 0.7735
04/22 01:37:23 AM: Epoch: 14/25, Batch: 8501/8584, Train Loss: 0.8763
04/22 01:37:23 AM: Epoch: 14/25, Train Loss: 0.8101
04/22 01:37:23 AM: Batch: 1/154, Loss: 0.7685, Acc: 64.0625
04/22 01:37:23 AM: Batch: 11/154, Loss: 0.7413, Acc: 64.0625
04/22 01:37:23 AM: Batch: 21/154, Loss: 0.8419, Acc: 59.3750
04/22 01:37:23 AM: Batch: 31/154, Loss: 0.6175, Acc: 78.1250
04/22 01:37:23 AM: Batch: 41/154, Loss: 0.7202, Acc: 75.0000
04/22 01:37:23 AM: Batch: 51/154, Loss: 0.8296, Acc: 60.9375
04/22 01:37:23 AM: Batch: 61/154, Loss: 0.8540, Acc: 56.2500
04/22 01:37:23 AM: Batch: 71/154, Loss: 0.8881, Acc: 60.9375
04/22 01:37:23 AM: Batch: 81/154, Loss: 0.8404, Acc: 67.1875
04/22 01:37:23 AM: Batch: 91/154, Loss: 0.7807, Acc: 67.1875
04/22 01:37:23 AM: Batch: 101/154, Loss: 0.7808, Acc: 67.1875
04/22 01:37:23 AM: Batch: 111/154, Loss: 0.6851, Acc: 73.4375
04/22 01:37:23 AM: Batch: 121/154, Loss: 0.7420, Acc: 68.7500
04/22 01:37:23 AM: Batch: 131/154, Loss: 0.9445, Acc: 62.5000
04/22 01:37:23 AM: Batch: 141/154, Loss: 0.7894, Acc: 70.3125
04/22 01:37:23 AM: Batch: 151/154, Loss: 0.7973, Acc: 68.7500
04/22 01:37:23 AM: Epoch: 14/25, Val Loss: 0.7933, Val Acc: 65.6879
04/22 01:37:23 AM: Best model found with val acc.: 65.6879
 56%|█████▌    | 14/25 [04:33<03:34, 19.53s/it]04/22 01:37:23 AM: Epoch: 15/25
Adjusting learning rate of group 0 to 8.6875e-02.
04/22 01:37:25 AM: Epoch: 15/25, Batch: 501/8584, Train Loss: 0.8120
04/22 01:37:26 AM: Epoch: 15/25, Batch: 1001/8584, Train Loss: 0.8565
04/22 01:37:27 AM: Epoch: 15/25, Batch: 1501/8584, Train Loss: 0.6839
04/22 01:37:28 AM: Epoch: 15/25, Batch: 2001/8584, Train Loss: 0.7982
04/22 01:37:29 AM: Epoch: 15/25, Batch: 2501/8584, Train Loss: 0.6945
04/22 01:37:30 AM: Epoch: 15/25, Batch: 3001/8584, Train Loss: 0.7406
04/22 01:37:31 AM: Epoch: 15/25, Batch: 3501/8584, Train Loss: 0.9066
04/22 01:37:33 AM: Epoch: 15/25, Batch: 4001/8584, Train Loss: 0.9051
04/22 01:37:34 AM: Epoch: 15/25, Batch: 4501/8584, Train Loss: 0.8464
04/22 01:37:35 AM: Epoch: 15/25, Batch: 5001/8584, Train Loss: 0.7359
04/22 01:37:36 AM: Epoch: 15/25, Batch: 5501/8584, Train Loss: 0.7293
04/22 01:37:37 AM: Epoch: 15/25, Batch: 6001/8584, Train Loss: 0.7292
04/22 01:37:38 AM: Epoch: 15/25, Batch: 6501/8584, Train Loss: 0.7401
04/22 01:37:39 AM: Epoch: 15/25, Batch: 7001/8584, Train Loss: 0.9016
04/22 01:37:40 AM: Epoch: 15/25, Batch: 7501/8584, Train Loss: 0.9198
04/22 01:37:41 AM: Epoch: 15/25, Batch: 8001/8584, Train Loss: 0.7724
04/22 01:37:42 AM: Epoch: 15/25, Batch: 8501/8584, Train Loss: 0.8752
04/22 01:37:43 AM: Epoch: 15/25, Train Loss: 0.8094
04/22 01:37:43 AM: Batch: 1/154, Loss: 0.7669, Acc: 65.6250
04/22 01:37:43 AM: Batch: 11/154, Loss: 0.7404, Acc: 64.0625
04/22 01:37:43 AM: Batch: 21/154, Loss: 0.8413, Acc: 59.3750
04/22 01:37:43 AM: Batch: 31/154, Loss: 0.6170, Acc: 78.1250
04/22 01:37:43 AM: Batch: 41/154, Loss: 0.7194, Acc: 75.0000
04/22 01:37:43 AM: Batch: 51/154, Loss: 0.8286, Acc: 60.9375
04/22 01:37:43 AM: Batch: 61/154, Loss: 0.8528, Acc: 56.2500
04/22 01:37:43 AM: Batch: 71/154, Loss: 0.8877, Acc: 60.9375
04/22 01:37:43 AM: Batch: 81/154, Loss: 0.8400, Acc: 67.1875
04/22 01:37:43 AM: Batch: 91/154, Loss: 0.7803, Acc: 67.1875
04/22 01:37:43 AM: Batch: 101/154, Loss: 0.7810, Acc: 67.1875
04/22 01:37:43 AM: Batch: 111/154, Loss: 0.6844, Acc: 73.4375
04/22 01:37:43 AM: Batch: 121/154, Loss: 0.7415, Acc: 68.7500
04/22 01:37:43 AM: Batch: 131/154, Loss: 0.9436, Acc: 62.5000
04/22 01:37:43 AM: Batch: 141/154, Loss: 0.7890, Acc: 70.3125
04/22 01:37:43 AM: Batch: 151/154, Loss: 0.7963, Acc: 67.1875
04/22 01:37:43 AM: Epoch: 15/25, Val Loss: 0.7928, Val Acc: 65.7387
04/22 01:37:43 AM: Best model found with val acc.: 65.7387
 60%|██████    | 15/25 [04:53<03:15, 19.57s/it]04/22 01:37:43 AM: Epoch: 16/25
Adjusting learning rate of group 0 to 8.6006e-02.
04/22 01:37:44 AM: Epoch: 16/25, Batch: 501/8584, Train Loss: 0.8115
04/22 01:37:46 AM: Epoch: 16/25, Batch: 1001/8584, Train Loss: 0.8567
04/22 01:37:47 AM: Epoch: 16/25, Batch: 1501/8584, Train Loss: 0.6845
04/22 01:37:48 AM: Epoch: 16/25, Batch: 2001/8584, Train Loss: 0.7984
04/22 01:37:49 AM: Epoch: 16/25, Batch: 2501/8584, Train Loss: 0.6937
04/22 01:37:50 AM: Epoch: 16/25, Batch: 3001/8584, Train Loss: 0.7401
04/22 01:37:51 AM: Epoch: 16/25, Batch: 3501/8584, Train Loss: 0.9053
04/22 01:37:52 AM: Epoch: 16/25, Batch: 4001/8584, Train Loss: 0.9048
04/22 01:37:53 AM: Epoch: 16/25, Batch: 4501/8584, Train Loss: 0.8461
04/22 01:37:54 AM: Epoch: 16/25, Batch: 5001/8584, Train Loss: 0.7351
04/22 01:37:55 AM: Epoch: 16/25, Batch: 5501/8584, Train Loss: 0.7284
04/22 01:37:56 AM: Epoch: 16/25, Batch: 6001/8584, Train Loss: 0.7286
04/22 01:37:58 AM: Epoch: 16/25, Batch: 6501/8584, Train Loss: 0.7403
04/22 01:37:59 AM: Epoch: 16/25, Batch: 7001/8584, Train Loss: 0.9005
04/22 01:38:00 AM: Epoch: 16/25, Batch: 7501/8584, Train Loss: 0.9195
04/22 01:38:01 AM: Epoch: 16/25, Batch: 8001/8584, Train Loss: 0.7714
04/22 01:38:02 AM: Epoch: 16/25, Batch: 8501/8584, Train Loss: 0.8743
04/22 01:38:02 AM: Epoch: 16/25, Train Loss: 0.8088
04/22 01:38:02 AM: Batch: 1/154, Loss: 0.7655, Acc: 65.6250
04/22 01:38:02 AM: Batch: 11/154, Loss: 0.7395, Acc: 65.6250
04/22 01:38:02 AM: Batch: 21/154, Loss: 0.8408, Acc: 59.3750
04/22 01:38:02 AM: Batch: 31/154, Loss: 0.6164, Acc: 79.6875
04/22 01:38:02 AM: Batch: 41/154, Loss: 0.7187, Acc: 75.0000
04/22 01:38:02 AM: Batch: 51/154, Loss: 0.8277, Acc: 60.9375
04/22 01:38:02 AM: Batch: 61/154, Loss: 0.8518, Acc: 56.2500
04/22 01:38:02 AM: Batch: 71/154, Loss: 0.8874, Acc: 60.9375
04/22 01:38:03 AM: Batch: 81/154, Loss: 0.8396, Acc: 67.1875
04/22 01:38:03 AM: Batch: 91/154, Loss: 0.7799, Acc: 65.6250
04/22 01:38:03 AM: Batch: 101/154, Loss: 0.7812, Acc: 67.1875
04/22 01:38:03 AM: Batch: 111/154, Loss: 0.6837, Acc: 73.4375
04/22 01:38:03 AM: Batch: 121/154, Loss: 0.7410, Acc: 68.7500
04/22 01:38:03 AM: Batch: 131/154, Loss: 0.9428, Acc: 62.5000
04/22 01:38:03 AM: Batch: 141/154, Loss: 0.7886, Acc: 70.3125
04/22 01:38:03 AM: Batch: 151/154, Loss: 0.7954, Acc: 67.1875
04/22 01:38:03 AM: Epoch: 16/25, Val Loss: 0.7922, Val Acc: 65.7793
04/22 01:38:03 AM: Best model found with val acc.: 65.7793
 64%|██████▍   | 16/25 [05:12<02:56, 19.57s/it]04/22 01:38:03 AM: Epoch: 17/25
Adjusting learning rate of group 0 to 8.5146e-02.
04/22 01:38:04 AM: Epoch: 17/25, Batch: 501/8584, Train Loss: 0.8110
04/22 01:38:05 AM: Epoch: 17/25, Batch: 1001/8584, Train Loss: 0.8568
04/22 01:38:06 AM: Epoch: 17/25, Batch: 1501/8584, Train Loss: 0.6851
04/22 01:38:07 AM: Epoch: 17/25, Batch: 2001/8584, Train Loss: 0.7987
04/22 01:38:08 AM: Epoch: 17/25, Batch: 2501/8584, Train Loss: 0.6929
04/22 01:38:10 AM: Epoch: 17/25, Batch: 3001/8584, Train Loss: 0.7396
04/22 01:38:11 AM: Epoch: 17/25, Batch: 3501/8584, Train Loss: 0.9041
04/22 01:38:12 AM: Epoch: 17/25, Batch: 4001/8584, Train Loss: 0.9045
04/22 01:38:13 AM: Epoch: 17/25, Batch: 4501/8584, Train Loss: 0.8459
04/22 01:38:14 AM: Epoch: 17/25, Batch: 5001/8584, Train Loss: 0.7343
04/22 01:38:15 AM: Epoch: 17/25, Batch: 5501/8584, Train Loss: 0.7276
04/22 01:38:16 AM: Epoch: 17/25, Batch: 6001/8584, Train Loss: 0.7279
04/22 01:38:17 AM: Epoch: 17/25, Batch: 6501/8584, Train Loss: 0.7404
04/22 01:38:18 AM: Epoch: 17/25, Batch: 7001/8584, Train Loss: 0.8994
04/22 01:38:19 AM: Epoch: 17/25, Batch: 7501/8584, Train Loss: 0.9193
04/22 01:38:20 AM: Epoch: 17/25, Batch: 8001/8584, Train Loss: 0.7705
04/22 01:38:22 AM: Epoch: 17/25, Batch: 8501/8584, Train Loss: 0.8734
04/22 01:38:22 AM: Epoch: 17/25, Train Loss: 0.8083
04/22 01:38:22 AM: Batch: 1/154, Loss: 0.7643, Acc: 65.6250
04/22 01:38:22 AM: Batch: 11/154, Loss: 0.7388, Acc: 65.6250
04/22 01:38:22 AM: Batch: 21/154, Loss: 0.8403, Acc: 59.3750
04/22 01:38:22 AM: Batch: 31/154, Loss: 0.6159, Acc: 79.6875
04/22 01:38:22 AM: Batch: 41/154, Loss: 0.7180, Acc: 75.0000
04/22 01:38:22 AM: Batch: 51/154, Loss: 0.8269, Acc: 60.9375
04/22 01:38:22 AM: Batch: 61/154, Loss: 0.8508, Acc: 56.2500
04/22 01:38:22 AM: Batch: 71/154, Loss: 0.8870, Acc: 59.3750
04/22 01:38:22 AM: Batch: 81/154, Loss: 0.8392, Acc: 67.1875
04/22 01:38:22 AM: Batch: 91/154, Loss: 0.7795, Acc: 67.1875
04/22 01:38:22 AM: Batch: 101/154, Loss: 0.7814, Acc: 67.1875
04/22 01:38:22 AM: Batch: 111/154, Loss: 0.6831, Acc: 73.4375
04/22 01:38:22 AM: Batch: 121/154, Loss: 0.7405, Acc: 68.7500
04/22 01:38:22 AM: Batch: 131/154, Loss: 0.9421, Acc: 62.5000
04/22 01:38:22 AM: Batch: 141/154, Loss: 0.7883, Acc: 70.3125
04/22 01:38:22 AM: Batch: 151/154, Loss: 0.7945, Acc: 67.1875
04/22 01:38:22 AM: Epoch: 17/25, Val Loss: 0.7917, Val Acc: 65.8200
04/22 01:38:22 AM: Best model found with val acc.: 65.8200
 68%|██████▊   | 17/25 [05:32<02:36, 19.59s/it]04/22 01:38:22 AM: Epoch: 18/25
Adjusting learning rate of group 0 to 8.4294e-02.
04/22 01:38:24 AM: Epoch: 18/25, Batch: 501/8584, Train Loss: 0.8105
04/22 01:38:25 AM: Epoch: 18/25, Batch: 1001/8584, Train Loss: 0.8569
04/22 01:38:26 AM: Epoch: 18/25, Batch: 1501/8584, Train Loss: 0.6857
04/22 01:38:27 AM: Epoch: 18/25, Batch: 2001/8584, Train Loss: 0.7990
04/22 01:38:28 AM: Epoch: 18/25, Batch: 2501/8584, Train Loss: 0.6922
04/22 01:38:29 AM: Epoch: 18/25, Batch: 3001/8584, Train Loss: 0.7391
04/22 01:38:30 AM: Epoch: 18/25, Batch: 3501/8584, Train Loss: 0.9029
04/22 01:38:31 AM: Epoch: 18/25, Batch: 4001/8584, Train Loss: 0.9043
04/22 01:38:32 AM: Epoch: 18/25, Batch: 4501/8584, Train Loss: 0.8457
04/22 01:38:34 AM: Epoch: 18/25, Batch: 5001/8584, Train Loss: 0.7336
04/22 01:38:35 AM: Epoch: 18/25, Batch: 5501/8584, Train Loss: 0.7268
04/22 01:38:36 AM: Epoch: 18/25, Batch: 6001/8584, Train Loss: 0.7273
04/22 01:38:37 AM: Epoch: 18/25, Batch: 6501/8584, Train Loss: 0.7406
04/22 01:38:38 AM: Epoch: 18/25, Batch: 7001/8584, Train Loss: 0.8984
04/22 01:38:39 AM: Epoch: 18/25, Batch: 7501/8584, Train Loss: 0.9190
04/22 01:38:40 AM: Epoch: 18/25, Batch: 8001/8584, Train Loss: 0.7698
04/22 01:38:41 AM: Epoch: 18/25, Batch: 8501/8584, Train Loss: 0.8726
04/22 01:38:41 AM: Epoch: 18/25, Train Loss: 0.8078
04/22 01:38:42 AM: Batch: 1/154, Loss: 0.7631, Acc: 64.0625
04/22 01:38:42 AM: Batch: 11/154, Loss: 0.7381, Acc: 65.6250
04/22 01:38:42 AM: Batch: 21/154, Loss: 0.8398, Acc: 59.3750
04/22 01:38:42 AM: Batch: 31/154, Loss: 0.6154, Acc: 79.6875
04/22 01:38:42 AM: Batch: 41/154, Loss: 0.7174, Acc: 75.0000
04/22 01:38:42 AM: Batch: 51/154, Loss: 0.8262, Acc: 60.9375
04/22 01:38:42 AM: Batch: 61/154, Loss: 0.8499, Acc: 56.2500
04/22 01:38:42 AM: Batch: 71/154, Loss: 0.8867, Acc: 59.3750
04/22 01:38:42 AM: Batch: 81/154, Loss: 0.8389, Acc: 67.1875
04/22 01:38:42 AM: Batch: 91/154, Loss: 0.7792, Acc: 67.1875
04/22 01:38:42 AM: Batch: 101/154, Loss: 0.7817, Acc: 67.1875
04/22 01:38:42 AM: Batch: 111/154, Loss: 0.6825, Acc: 73.4375
04/22 01:38:42 AM: Batch: 121/154, Loss: 0.7399, Acc: 68.7500
04/22 01:38:42 AM: Batch: 131/154, Loss: 0.9414, Acc: 62.5000
04/22 01:38:42 AM: Batch: 141/154, Loss: 0.7880, Acc: 70.3125
04/22 01:38:42 AM: Batch: 151/154, Loss: 0.7937, Acc: 68.7500
04/22 01:38:42 AM: Epoch: 18/25, Val Loss: 0.7912, Val Acc: 65.8809
04/22 01:38:42 AM: Best model found with val acc.: 65.8809
 72%|███████▏  | 18/25 [05:52<02:17, 19.61s/it]04/22 01:38:42 AM: Epoch: 19/25
Adjusting learning rate of group 0 to 8.3451e-02.
04/22 01:38:43 AM: Epoch: 19/25, Batch: 501/8584, Train Loss: 0.8100
04/22 01:38:44 AM: Epoch: 19/25, Batch: 1001/8584, Train Loss: 0.8570
04/22 01:38:46 AM: Epoch: 19/25, Batch: 1501/8584, Train Loss: 0.6862
04/22 01:38:47 AM: Epoch: 19/25, Batch: 2001/8584, Train Loss: 0.7993
04/22 01:38:48 AM: Epoch: 19/25, Batch: 2501/8584, Train Loss: 0.6915
04/22 01:38:49 AM: Epoch: 19/25, Batch: 3001/8584, Train Loss: 0.7386
04/22 01:38:50 AM: Epoch: 19/25, Batch: 3501/8584, Train Loss: 0.9018
04/22 01:38:51 AM: Epoch: 19/25, Batch: 4001/8584, Train Loss: 0.9041
04/22 01:38:52 AM: Epoch: 19/25, Batch: 4501/8584, Train Loss: 0.8455
04/22 01:38:53 AM: Epoch: 19/25, Batch: 5001/8584, Train Loss: 0.7330
04/22 01:38:54 AM: Epoch: 19/25, Batch: 5501/8584, Train Loss: 0.7261
04/22 01:38:56 AM: Epoch: 19/25, Batch: 6001/8584, Train Loss: 0.7267
04/22 01:38:57 AM: Epoch: 19/25, Batch: 6501/8584, Train Loss: 0.7409
04/22 01:38:58 AM: Epoch: 19/25, Batch: 7001/8584, Train Loss: 0.8975
04/22 01:38:59 AM: Epoch: 19/25, Batch: 7501/8584, Train Loss: 0.9188
04/22 01:39:00 AM: Epoch: 19/25, Batch: 8001/8584, Train Loss: 0.7691
04/22 01:39:01 AM: Epoch: 19/25, Batch: 8501/8584, Train Loss: 0.8718
04/22 01:39:01 AM: Epoch: 19/25, Train Loss: 0.8074
04/22 01:39:01 AM: Batch: 1/154, Loss: 0.7621, Acc: 64.0625
04/22 01:39:01 AM: Batch: 11/154, Loss: 0.7374, Acc: 65.6250
04/22 01:39:02 AM: Batch: 21/154, Loss: 0.8394, Acc: 59.3750
04/22 01:39:02 AM: Batch: 31/154, Loss: 0.6149, Acc: 78.1250
04/22 01:39:02 AM: Batch: 41/154, Loss: 0.7169, Acc: 75.0000
04/22 01:39:02 AM: Batch: 51/154, Loss: 0.8255, Acc: 60.9375
04/22 01:39:02 AM: Batch: 61/154, Loss: 0.8491, Acc: 56.2500
04/22 01:39:02 AM: Batch: 71/154, Loss: 0.8863, Acc: 59.3750
04/22 01:39:02 AM: Batch: 81/154, Loss: 0.8385, Acc: 67.1875
04/22 01:39:02 AM: Batch: 91/154, Loss: 0.7788, Acc: 67.1875
04/22 01:39:02 AM: Batch: 101/154, Loss: 0.7819, Acc: 67.1875
04/22 01:39:02 AM: Batch: 111/154, Loss: 0.6820, Acc: 73.4375
04/22 01:39:02 AM: Batch: 121/154, Loss: 0.7394, Acc: 68.7500
04/22 01:39:02 AM: Batch: 131/154, Loss: 0.9407, Acc: 62.5000
04/22 01:39:02 AM: Batch: 141/154, Loss: 0.7877, Acc: 70.3125
04/22 01:39:02 AM: Batch: 151/154, Loss: 0.7929, Acc: 68.7500
04/22 01:39:02 AM: Epoch: 19/25, Val Loss: 0.7907, Val Acc: 65.8809
04/22 01:39:02 AM: Learning rate decreased to: 0.0165
 76%|███████▌  | 19/25 [06:12<01:57, 19.67s/it]04/22 01:39:02 AM: Epoch: 20/25
Adjusting learning rate of group 0 to 8.2617e-02.
04/22 01:39:03 AM: Epoch: 20/25, Batch: 501/8584, Train Loss: 0.8088
04/22 01:39:04 AM: Epoch: 20/25, Batch: 1001/8584, Train Loss: 0.8639
04/22 01:39:05 AM: Epoch: 20/25, Batch: 1501/8584, Train Loss: 0.6889
04/22 01:39:06 AM: Epoch: 20/25, Batch: 2001/8584, Train Loss: 0.8162
04/22 01:39:07 AM: Epoch: 20/25, Batch: 2501/8584, Train Loss: 0.6750
04/22 01:39:08 AM: Epoch: 20/25, Batch: 3001/8584, Train Loss: 0.7258
04/22 01:39:10 AM: Epoch: 20/25, Batch: 3501/8584, Train Loss: 0.8973
04/22 01:39:11 AM: Epoch: 20/25, Batch: 4001/8584, Train Loss: 0.9016
04/22 01:39:12 AM: Epoch: 20/25, Batch: 4501/8584, Train Loss: 0.8321
04/22 01:39:13 AM: Epoch: 20/25, Batch: 5001/8584, Train Loss: 0.7369
04/22 01:39:14 AM: Epoch: 20/25, Batch: 5501/8584, Train Loss: 0.7193
04/22 01:39:15 AM: Epoch: 20/25, Batch: 6001/8584, Train Loss: 0.7178
04/22 01:39:16 AM: Epoch: 20/25, Batch: 6501/8584, Train Loss: 0.7519
04/22 01:39:17 AM: Epoch: 20/25, Batch: 7001/8584, Train Loss: 0.9029
04/22 01:39:18 AM: Epoch: 20/25, Batch: 7501/8584, Train Loss: 0.9148
04/22 01:39:19 AM: Epoch: 20/25, Batch: 8001/8584, Train Loss: 0.7609
04/22 01:39:20 AM: Epoch: 20/25, Batch: 8501/8584, Train Loss: 0.8689
04/22 01:39:21 AM: Epoch: 20/25, Train Loss: 0.8009
04/22 01:39:21 AM: Batch: 1/154, Loss: 0.7573, Acc: 64.0625
04/22 01:39:21 AM: Batch: 11/154, Loss: 0.7307, Acc: 68.7500
04/22 01:39:21 AM: Batch: 21/154, Loss: 0.8368, Acc: 54.6875
04/22 01:39:21 AM: Batch: 31/154, Loss: 0.6000, Acc: 79.6875
04/22 01:39:21 AM: Batch: 41/154, Loss: 0.7261, Acc: 73.4375
04/22 01:39:21 AM: Batch: 51/154, Loss: 0.8193, Acc: 60.9375
04/22 01:39:21 AM: Batch: 61/154, Loss: 0.8491, Acc: 56.2500
04/22 01:39:21 AM: Batch: 71/154, Loss: 0.8819, Acc: 56.2500
04/22 01:39:21 AM: Batch: 81/154, Loss: 0.8262, Acc: 67.1875
04/22 01:39:21 AM: Batch: 91/154, Loss: 0.7693, Acc: 71.8750
04/22 01:39:21 AM: Batch: 101/154, Loss: 0.7995, Acc: 65.6250
04/22 01:39:21 AM: Batch: 111/154, Loss: 0.6753, Acc: 70.3125
04/22 01:39:21 AM: Batch: 121/154, Loss: 0.7230, Acc: 70.3125
04/22 01:39:21 AM: Batch: 131/154, Loss: 0.9429, Acc: 57.8125
04/22 01:39:21 AM: Batch: 141/154, Loss: 0.7836, Acc: 68.7500
04/22 01:39:21 AM: Batch: 151/154, Loss: 0.7632, Acc: 71.8750
04/22 01:39:21 AM: Epoch: 20/25, Val Loss: 0.7844, Val Acc: 65.9419
04/22 01:39:21 AM: Best model found with val acc.: 65.9419
 80%|████████  | 20/25 [06:31<01:38, 19.60s/it]04/22 01:39:21 AM: Epoch: 21/25
Adjusting learning rate of group 0 to 1.6358e-02.
04/22 01:39:23 AM: Epoch: 21/25, Batch: 501/8584, Train Loss: 0.8063
04/22 01:39:24 AM: Epoch: 21/25, Batch: 1001/8584, Train Loss: 0.8656
04/22 01:39:25 AM: Epoch: 21/25, Batch: 1501/8584, Train Loss: 0.6870
04/22 01:39:26 AM: Epoch: 21/25, Batch: 2001/8584, Train Loss: 0.8189
04/22 01:39:27 AM: Epoch: 21/25, Batch: 2501/8584, Train Loss: 0.6748
04/22 01:39:28 AM: Epoch: 21/25, Batch: 3001/8584, Train Loss: 0.7251
04/22 01:39:29 AM: Epoch: 21/25, Batch: 3501/8584, Train Loss: 0.8962
04/22 01:39:30 AM: Epoch: 21/25, Batch: 4001/8584, Train Loss: 0.9011
04/22 01:39:31 AM: Epoch: 21/25, Batch: 4501/8584, Train Loss: 0.8328
04/22 01:39:32 AM: Epoch: 21/25, Batch: 5001/8584, Train Loss: 0.7375
04/22 01:39:34 AM: Epoch: 21/25, Batch: 5501/8584, Train Loss: 0.7189
04/22 01:39:35 AM: Epoch: 21/25, Batch: 6001/8584, Train Loss: 0.7181
04/22 01:39:36 AM: Epoch: 21/25, Batch: 6501/8584, Train Loss: 0.7524
04/22 01:39:37 AM: Epoch: 21/25, Batch: 7001/8584, Train Loss: 0.9016
04/22 01:39:38 AM: Epoch: 21/25, Batch: 7501/8584, Train Loss: 0.9159
04/22 01:39:39 AM: Epoch: 21/25, Batch: 8001/8584, Train Loss: 0.7602
04/22 01:39:40 AM: Epoch: 21/25, Batch: 8501/8584, Train Loss: 0.8686
04/22 01:39:40 AM: Epoch: 21/25, Train Loss: 0.8007
04/22 01:39:41 AM: Batch: 1/154, Loss: 0.7560, Acc: 64.0625
04/22 01:39:41 AM: Batch: 11/154, Loss: 0.7297, Acc: 68.7500
04/22 01:39:41 AM: Batch: 21/154, Loss: 0.8366, Acc: 54.6875
04/22 01:39:41 AM: Batch: 31/154, Loss: 0.6007, Acc: 78.1250
04/22 01:39:41 AM: Batch: 41/154, Loss: 0.7252, Acc: 73.4375
04/22 01:39:41 AM: Batch: 51/154, Loss: 0.8181, Acc: 60.9375
04/22 01:39:41 AM: Batch: 61/154, Loss: 0.8476, Acc: 56.2500
04/22 01:39:41 AM: Batch: 71/154, Loss: 0.8827, Acc: 56.2500
04/22 01:39:41 AM: Batch: 81/154, Loss: 0.8267, Acc: 67.1875
04/22 01:39:41 AM: Batch: 91/154, Loss: 0.7697, Acc: 71.8750
04/22 01:39:41 AM: Batch: 101/154, Loss: 0.8003, Acc: 65.6250
04/22 01:39:41 AM: Batch: 111/154, Loss: 0.6753, Acc: 70.3125
04/22 01:39:41 AM: Batch: 121/154, Loss: 0.7239, Acc: 70.3125
04/22 01:39:41 AM: Batch: 131/154, Loss: 0.9429, Acc: 56.2500
04/22 01:39:41 AM: Batch: 141/154, Loss: 0.7832, Acc: 68.7500
04/22 01:39:41 AM: Batch: 151/154, Loss: 0.7634, Acc: 71.8750
04/22 01:39:41 AM: Epoch: 21/25, Val Loss: 0.7842, Val Acc: 65.9927
04/22 01:39:41 AM: Best model found with val acc.: 65.9927
 84%|████████▍ | 21/25 [06:51<01:18, 19.64s/it]04/22 01:39:41 AM: Epoch: 22/25
Adjusting learning rate of group 0 to 1.6195e-02.
04/22 01:39:42 AM: Epoch: 22/25, Batch: 501/8584, Train Loss: 0.8059
04/22 01:39:43 AM: Epoch: 22/25, Batch: 1001/8584, Train Loss: 0.8657
04/22 01:39:45 AM: Epoch: 22/25, Batch: 1501/8584, Train Loss: 0.6868
04/22 01:39:46 AM: Epoch: 22/25, Batch: 2001/8584, Train Loss: 0.8192
04/22 01:39:47 AM: Epoch: 22/25, Batch: 2501/8584, Train Loss: 0.6745
04/22 01:39:48 AM: Epoch: 22/25, Batch: 3001/8584, Train Loss: 0.7251
04/22 01:39:49 AM: Epoch: 22/25, Batch: 3501/8584, Train Loss: 0.8954
04/22 01:39:50 AM: Epoch: 22/25, Batch: 4001/8584, Train Loss: 0.9006
04/22 01:39:51 AM: Epoch: 22/25, Batch: 4501/8584, Train Loss: 0.8329
04/22 01:39:52 AM: Epoch: 22/25, Batch: 5001/8584, Train Loss: 0.7377
04/22 01:39:53 AM: Epoch: 22/25, Batch: 5501/8584, Train Loss: 0.7186
04/22 01:39:54 AM: Epoch: 22/25, Batch: 6001/8584, Train Loss: 0.7183
04/22 01:39:55 AM: Epoch: 22/25, Batch: 6501/8584, Train Loss: 0.7527
04/22 01:39:57 AM: Epoch: 22/25, Batch: 7001/8584, Train Loss: 0.9007
04/22 01:39:58 AM: Epoch: 22/25, Batch: 7501/8584, Train Loss: 0.9165
04/22 01:39:59 AM: Epoch: 22/25, Batch: 8001/8584, Train Loss: 0.7598
04/22 01:40:00 AM: Epoch: 22/25, Batch: 8501/8584, Train Loss: 0.8683
04/22 01:40:00 AM: Epoch: 22/25, Train Loss: 0.8006
04/22 01:40:00 AM: Batch: 1/154, Loss: 0.7553, Acc: 64.0625
04/22 01:40:00 AM: Batch: 11/154, Loss: 0.7293, Acc: 68.7500
04/22 01:40:00 AM: Batch: 21/154, Loss: 0.8366, Acc: 54.6875
04/22 01:40:00 AM: Batch: 31/154, Loss: 0.6011, Acc: 78.1250
04/22 01:40:00 AM: Batch: 41/154, Loss: 0.7248, Acc: 73.4375
04/22 01:40:00 AM: Batch: 51/154, Loss: 0.8174, Acc: 60.9375
04/22 01:40:00 AM: Batch: 61/154, Loss: 0.8465, Acc: 56.2500
04/22 01:40:00 AM: Batch: 71/154, Loss: 0.8831, Acc: 56.2500
04/22 01:40:00 AM: Batch: 81/154, Loss: 0.8269, Acc: 68.7500
04/22 01:40:00 AM: Batch: 91/154, Loss: 0.7698, Acc: 71.8750
04/22 01:40:00 AM: Batch: 101/154, Loss: 0.8007, Acc: 65.6250
04/22 01:40:00 AM: Batch: 111/154, Loss: 0.6752, Acc: 70.3125
04/22 01:40:00 AM: Batch: 121/154, Loss: 0.7244, Acc: 70.3125
04/22 01:40:00 AM: Batch: 131/154, Loss: 0.9426, Acc: 56.2500
04/22 01:40:00 AM: Batch: 141/154, Loss: 0.7830, Acc: 68.7500
04/22 01:40:00 AM: Batch: 151/154, Loss: 0.7635, Acc: 71.8750
04/22 01:40:01 AM: Epoch: 22/25, Val Loss: 0.7841, Val Acc: 66.0435
04/22 01:40:01 AM: Best model found with val acc.: 66.0435
 88%|████████▊ | 22/25 [07:10<00:58, 19.63s/it]04/22 01:40:01 AM: Epoch: 23/25
Adjusting learning rate of group 0 to 1.6033e-02.
04/22 01:40:02 AM: Epoch: 23/25, Batch: 501/8584, Train Loss: 0.8058
04/22 01:40:03 AM: Epoch: 23/25, Batch: 1001/8584, Train Loss: 0.8656
04/22 01:40:04 AM: Epoch: 23/25, Batch: 1501/8584, Train Loss: 0.6866
04/22 01:40:05 AM: Epoch: 23/25, Batch: 2001/8584, Train Loss: 0.8192
04/22 01:40:06 AM: Epoch: 23/25, Batch: 2501/8584, Train Loss: 0.6743
04/22 01:40:07 AM: Epoch: 23/25, Batch: 3001/8584, Train Loss: 0.7252
04/22 01:40:09 AM: Epoch: 23/25, Batch: 3501/8584, Train Loss: 0.8948
04/22 01:40:10 AM: Epoch: 23/25, Batch: 4001/8584, Train Loss: 0.9003
04/22 01:40:11 AM: Epoch: 23/25, Batch: 4501/8584, Train Loss: 0.8329
04/22 01:40:12 AM: Epoch: 23/25, Batch: 5001/8584, Train Loss: 0.7377
04/22 01:40:13 AM: Epoch: 23/25, Batch: 5501/8584, Train Loss: 0.7183
04/22 01:40:14 AM: Epoch: 23/25, Batch: 6001/8584, Train Loss: 0.7184
04/22 01:40:15 AM: Epoch: 23/25, Batch: 6501/8584, Train Loss: 0.7529
04/22 01:40:16 AM: Epoch: 23/25, Batch: 7001/8584, Train Loss: 0.9001
04/22 01:40:17 AM: Epoch: 23/25, Batch: 7501/8584, Train Loss: 0.9170
04/22 01:40:18 AM: Epoch: 23/25, Batch: 8001/8584, Train Loss: 0.7594
04/22 01:40:19 AM: Epoch: 23/25, Batch: 8501/8584, Train Loss: 0.8680
04/22 01:40:20 AM: Epoch: 23/25, Train Loss: 0.8005
04/22 01:40:20 AM: Batch: 1/154, Loss: 0.7547, Acc: 64.0625
04/22 01:40:20 AM: Batch: 11/154, Loss: 0.7291, Acc: 68.7500
04/22 01:40:20 AM: Batch: 21/154, Loss: 0.8366, Acc: 54.6875
04/22 01:40:20 AM: Batch: 31/154, Loss: 0.6013, Acc: 78.1250
04/22 01:40:20 AM: Batch: 41/154, Loss: 0.7244, Acc: 73.4375
04/22 01:40:20 AM: Batch: 51/154, Loss: 0.8169, Acc: 60.9375
04/22 01:40:20 AM: Batch: 61/154, Loss: 0.8457, Acc: 56.2500
04/22 01:40:20 AM: Batch: 71/154, Loss: 0.8833, Acc: 56.2500
04/22 01:40:20 AM: Batch: 81/154, Loss: 0.8269, Acc: 68.7500
04/22 01:40:20 AM: Batch: 91/154, Loss: 0.7698, Acc: 71.8750
04/22 01:40:20 AM: Batch: 101/154, Loss: 0.8010, Acc: 65.6250
04/22 01:40:20 AM: Batch: 111/154, Loss: 0.6752, Acc: 70.3125
04/22 01:40:20 AM: Batch: 121/154, Loss: 0.7248, Acc: 70.3125
04/22 01:40:20 AM: Batch: 131/154, Loss: 0.9423, Acc: 57.8125
04/22 01:40:20 AM: Batch: 141/154, Loss: 0.7827, Acc: 68.7500
04/22 01:40:20 AM: Batch: 151/154, Loss: 0.7635, Acc: 71.8750
04/22 01:40:20 AM: Epoch: 23/25, Val Loss: 0.7840, Val Acc: 66.0333
04/22 01:40:20 AM: Learning rate decreased to: 0.0032
 92%|█████████▏| 23/25 [07:30<00:39, 19.61s/it]04/22 01:40:20 AM: Epoch: 24/25
Adjusting learning rate of group 0 to 1.5872e-02.
04/22 01:40:21 AM: Epoch: 24/25, Batch: 501/8584, Train Loss: 0.8037
04/22 01:40:23 AM: Epoch: 24/25, Batch: 1001/8584, Train Loss: 0.8641
04/22 01:40:24 AM: Epoch: 24/25, Batch: 1501/8584, Train Loss: 0.6847
04/22 01:40:25 AM: Epoch: 24/25, Batch: 2001/8584, Train Loss: 0.8175
04/22 01:40:26 AM: Epoch: 24/25, Batch: 2501/8584, Train Loss: 0.6720
04/22 01:40:27 AM: Epoch: 24/25, Batch: 3001/8584, Train Loss: 0.7212
04/22 01:40:28 AM: Epoch: 24/25, Batch: 3501/8584, Train Loss: 0.8951
04/22 01:40:29 AM: Epoch: 24/25, Batch: 4001/8584, Train Loss: 0.8997
04/22 01:40:30 AM: Epoch: 24/25, Batch: 4501/8584, Train Loss: 0.8364
04/22 01:40:31 AM: Epoch: 24/25, Batch: 5001/8584, Train Loss: 0.7386
04/22 01:40:32 AM: Epoch: 24/25, Batch: 5501/8584, Train Loss: 0.7189
04/22 01:40:33 AM: Epoch: 24/25, Batch: 6001/8584, Train Loss: 0.7145
04/22 01:40:34 AM: Epoch: 24/25, Batch: 6501/8584, Train Loss: 0.7462
04/22 01:40:36 AM: Epoch: 24/25, Batch: 7001/8584, Train Loss: 0.9034
04/22 01:40:37 AM: Epoch: 24/25, Batch: 7501/8584, Train Loss: 0.9189
04/22 01:40:38 AM: Epoch: 24/25, Batch: 8001/8584, Train Loss: 0.7568
04/22 01:40:39 AM: Epoch: 24/25, Batch: 8501/8584, Train Loss: 0.8673
04/22 01:40:39 AM: Epoch: 24/25, Train Loss: 0.7996
04/22 01:40:39 AM: Batch: 1/154, Loss: 0.7540, Acc: 64.0625
04/22 01:40:39 AM: Batch: 11/154, Loss: 0.7281, Acc: 68.7500
04/22 01:40:39 AM: Batch: 21/154, Loss: 0.8359, Acc: 56.2500
04/22 01:40:39 AM: Batch: 31/154, Loss: 0.6027, Acc: 78.1250
04/22 01:40:39 AM: Batch: 41/154, Loss: 0.7169, Acc: 73.4375
04/22 01:40:39 AM: Batch: 51/154, Loss: 0.8181, Acc: 60.9375
04/22 01:40:39 AM: Batch: 61/154, Loss: 0.8406, Acc: 56.2500
04/22 01:40:39 AM: Batch: 71/154, Loss: 0.8812, Acc: 56.2500
04/22 01:40:39 AM: Batch: 81/154, Loss: 0.8277, Acc: 68.7500
04/22 01:40:39 AM: Batch: 91/154, Loss: 0.7731, Acc: 68.7500
04/22 01:40:39 AM: Batch: 101/154, Loss: 0.7951, Acc: 68.7500
04/22 01:40:39 AM: Batch: 111/154, Loss: 0.6731, Acc: 71.8750
04/22 01:40:39 AM: Batch: 121/154, Loss: 0.7259, Acc: 68.7500
04/22 01:40:39 AM: Batch: 131/154, Loss: 0.9434, Acc: 59.3750
04/22 01:40:39 AM: Batch: 141/154, Loss: 0.7814, Acc: 67.1875
04/22 01:40:39 AM: Batch: 151/154, Loss: 0.7647, Acc: 70.3125
04/22 01:40:40 AM: Epoch: 24/25, Val Loss: 0.7840, Val Acc: 66.0536
04/22 01:40:40 AM: Best model found with val acc.: 66.0536
 96%|█████████▌| 24/25 [07:49<00:19, 19.56s/it]04/22 01:40:40 AM: Epoch: 25/25
Adjusting learning rate of group 0 to 3.1427e-03.
04/22 01:40:41 AM: Epoch: 25/25, Batch: 501/8584, Train Loss: 0.8037
04/22 01:40:42 AM: Epoch: 25/25, Batch: 1001/8584, Train Loss: 0.8641
04/22 01:40:43 AM: Epoch: 25/25, Batch: 1501/8584, Train Loss: 0.6848
04/22 01:40:44 AM: Epoch: 25/25, Batch: 2001/8584, Train Loss: 0.8178
04/22 01:40:45 AM: Epoch: 25/25, Batch: 2501/8584, Train Loss: 0.6719
04/22 01:40:46 AM: Epoch: 25/25, Batch: 3001/8584, Train Loss: 0.7214
04/22 01:40:47 AM: Epoch: 25/25, Batch: 3501/8584, Train Loss: 0.8949
04/22 01:40:49 AM: Epoch: 25/25, Batch: 4001/8584, Train Loss: 0.8993
04/22 01:40:50 AM: Epoch: 25/25, Batch: 4501/8584, Train Loss: 0.8363
04/22 01:40:51 AM: Epoch: 25/25, Batch: 5001/8584, Train Loss: 0.7388
04/22 01:40:52 AM: Epoch: 25/25, Batch: 5501/8584, Train Loss: 0.7193
04/22 01:40:53 AM: Epoch: 25/25, Batch: 6001/8584, Train Loss: 0.7148
04/22 01:40:54 AM: Epoch: 25/25, Batch: 6501/8584, Train Loss: 0.7465
04/22 01:40:55 AM: Epoch: 25/25, Batch: 7001/8584, Train Loss: 0.9034
04/22 01:40:56 AM: Epoch: 25/25, Batch: 7501/8584, Train Loss: 0.9191
04/22 01:40:57 AM: Epoch: 25/25, Batch: 8001/8584, Train Loss: 0.7567
04/22 01:40:58 AM: Epoch: 25/25, Batch: 8501/8584, Train Loss: 0.8669
04/22 01:40:58 AM: Epoch: 25/25, Train Loss: 0.7995
04/22 01:40:59 AM: Batch: 1/154, Loss: 0.7537, Acc: 64.0625
04/22 01:40:59 AM: Batch: 11/154, Loss: 0.7275, Acc: 70.3125
04/22 01:40:59 AM: Batch: 21/154, Loss: 0.8354, Acc: 56.2500
04/22 01:40:59 AM: Batch: 31/154, Loss: 0.6030, Acc: 78.1250
04/22 01:40:59 AM: Batch: 41/154, Loss: 0.7166, Acc: 73.4375
04/22 01:40:59 AM: Batch: 51/154, Loss: 0.8180, Acc: 60.9375
04/22 01:40:59 AM: Batch: 61/154, Loss: 0.8401, Acc: 56.2500
04/22 01:40:59 AM: Batch: 71/154, Loss: 0.8811, Acc: 56.2500
04/22 01:40:59 AM: Batch: 81/154, Loss: 0.8278, Acc: 70.3125
04/22 01:40:59 AM: Batch: 91/154, Loss: 0.7733, Acc: 68.7500
04/22 01:40:59 AM: Batch: 101/154, Loss: 0.7949, Acc: 68.7500
04/22 01:40:59 AM: Batch: 111/154, Loss: 0.6731, Acc: 71.8750
04/22 01:40:59 AM: Batch: 121/154, Loss: 0.7259, Acc: 68.7500
04/22 01:40:59 AM: Batch: 131/154, Loss: 0.9436, Acc: 59.3750
04/22 01:40:59 AM: Batch: 141/154, Loss: 0.7815, Acc: 67.1875
04/22 01:40:59 AM: Batch: 151/154, Loss: 0.7646, Acc: 70.3125
04/22 01:40:59 AM: Epoch: 25/25, Val Loss: 0.7840, Val Acc: 66.0435
04/22 01:40:59 AM: Learning rate decreased to: 0.0006
100%|██████████| 25/25 [08:09<00:00, 19.53s/it]100%|██████████| 25/25 [08:09<00:00, 19.57s/it]
04/22 01:40:59 AM: Best val loss: 0.7840
04/22 01:40:59 AM: Best val acc: 66.0536
04/22 01:40:59 AM: Best validation loss: 0.7840, Best validation accuracy: 0.6605
04/22 01:40:59 AM: Loading the best model...
Adjusting learning rate of group 0 to 3.1113e-03.
04/22 01:40:59 AM: Batch: 1/154, Loss: 0.7939, Acc: 64.0625
04/22 01:40:59 AM: Batch: 11/154, Loss: 0.7908, Acc: 64.0625
04/22 01:40:59 AM: Batch: 21/154, Loss: 0.7190, Acc: 76.5625
04/22 01:40:59 AM: Batch: 31/154, Loss: 0.7624, Acc: 70.3125
04/22 01:40:59 AM: Batch: 41/154, Loss: 0.7618, Acc: 60.9375
04/22 01:40:59 AM: Batch: 51/154, Loss: 0.8279, Acc: 62.5000
04/22 01:40:59 AM: Batch: 61/154, Loss: 0.7173, Acc: 65.6250
04/22 01:40:59 AM: Batch: 71/154, Loss: 0.7320, Acc: 67.1875
04/22 01:40:59 AM: Batch: 81/154, Loss: 0.8302, Acc: 64.0625
04/22 01:40:59 AM: Batch: 91/154, Loss: 0.7427, Acc: 68.7500
04/22 01:40:59 AM: Batch: 101/154, Loss: 0.8435, Acc: 65.6250
04/22 01:40:59 AM: Batch: 111/154, Loss: 0.8206, Acc: 65.6250
04/22 01:41:00 AM: Batch: 121/154, Loss: 0.7490, Acc: 67.1875
04/22 01:41:00 AM: Batch: 131/154, Loss: 0.8047, Acc: 67.1875
04/22 01:41:00 AM: Batch: 141/154, Loss: 0.8268, Acc: 67.1875
04/22 01:41:00 AM: Batch: 151/154, Loss: 0.8763, Acc: 62.5000
04/22 01:41:00 AM: Test loss: 0.7885, Test accuracy: 66.1950
04/22 01:41:00 AM: Done!

JOB STATISTICS
==============
Job ID: 5996547
Cluster: snellius
User/Group: scur1398/scur1398
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 02:34:48 core-walltime
Job Wall-clock time: 00:08:36
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.
